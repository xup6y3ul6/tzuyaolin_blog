[
  {
    "objectID": "research/publication.html",
    "href": "research/publication.html",
    "title": "Publication",
    "section": "",
    "text": "Lin, T.-Y., Tuerlinckx F., Vanbelle, S.* (In progress). Reliability in intensive longitudinal studies: A state-space vs.linear mixed model approach.\nLo, C.*, Lin, T.-Y., & XXXX (In progress). VHS\nRao, Y., Yu, E. Y.-W., Lin, T.-Y., Chen, Y., Qin, Y.,Koster A., Eussen, S., Bosma, H., Berendschot, T.J.M, van der Kallen, C. , van Greevenbroek M., de Galan, B., Zeegers, M., Wesselius A. (Submitted). Interaction between coffee consumption and polygenic risk score in relation to diabetes: Insights from The Maastricht Study."
  },
  {
    "objectID": "research/publication.html#working-paper",
    "href": "research/publication.html#working-paper",
    "title": "Publication",
    "section": "",
    "text": "Lin, T.-Y., Tuerlinckx F., Vanbelle, S.* (In progress). Reliability in intensive longitudinal studies: A state-space vs.linear mixed model approach.\nLo, C.*, Lin, T.-Y., & XXXX (In progress). VHS\nRao, Y., Yu, E. Y.-W., Lin, T.-Y., Chen, Y., Qin, Y.,Koster A., Eussen, S., Bosma, H., Berendschot, T.J.M, van der Kallen, C. , van Greevenbroek M., de Galan, B., Zeegers, M., Wesselius A. (Submitted). Interaction between coffee consumption and polygenic risk score in relation to diabetes: Insights from The Maastricht Study."
  },
  {
    "objectID": "research/publication.html#published",
    "href": "research/publication.html#published",
    "title": "Publication",
    "section": "Published",
    "text": "Published\nLin, T.-Y., Tuerlinckx F., & Vanbelle, S.* (2025). Reliability for multilevel data: A correlation approach. Psychological Methods, X(X), XX-XX. https://doi.org/10.1037/met0000738\nYu, K.*, Lin, T.-Y., Zaman, J., Tuerlinckx, F., & Vanhasbroeck, N. (2025). Consistency of perceptual response variability in size estimation and reproduction tasks. Behavior Research Methods, 57(5), 127. https://doi.org/10.3758/s13428-025-02650-1"
  },
  {
    "objectID": "research/publication.html#master-thesis",
    "href": "research/publication.html#master-thesis",
    "title": "Publication",
    "section": "Master thesis",
    "text": "Master thesis\nLin T.-Y., & Hsu Y.-F. (2022). Incorporating the Luce-Krantz threshold model into the cultural consensus theory for ordinal categorical data: A simulation study."
  },
  {
    "objectID": "projects/20230703_welcome/index.html",
    "href": "projects/20230703_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog, where I’ll be sharing my academic research progress, including my papers, study notes, and my work with R language, statistics, and psychology. Of course, I’ll also be sharing my personal thoughts and experiences.\nSo, why I want to build a personal blog. The reason is simple. It’s time to share what I know!\n\n\n\nWhen you’ve written the same code 3 times, write a functionWhen you’ve given the same in-person advice 3 times, write a blog post\n\n— David Robinson (@drob) November 9, 2017\n\n\n\nAlthough I haven’t quite figured out what this blog will look like yet, let’s get started and worry about the details later. Anyway, thanks for joining me on this journey!\n\n\n\nThe sunrise in Kinderdijk (Children dike), the Netherlands"
  },
  {
    "objectID": "posts/20250330_PhD-position/index.html",
    "href": "posts/20250330_PhD-position/index.html",
    "title": "Resource to find a PhD (or researcher) position",
    "section": "",
    "text": "先從你感興趣的領域獲取資訊\n\nallstat@JISCMAIL.AC.UK (https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=allstat) 統計相關的職缺，可能會有實驗室招博士生的訊息\nMathPsych (https://mathpsych.org/page/mailing-lists) 數理心理學、心理計量等的博士缺\nPsycCareers (https://www.psyccareers.com/) 主要是提供心理學家在美國的工作缺"
  },
  {
    "objectID": "posts/20250330_PhD-position/index.html#overall-in-euro",
    "href": "posts/20250330_PhD-position/index.html#overall-in-euro",
    "title": "Resource to find a PhD (or researcher) position",
    "section": "Overall in Euro",
    "text": "Overall in Euro\n\nEURAXESS\nMarie Skłodowska-Curie Actions"
  },
  {
    "objectID": "posts/20250330_PhD-position/index.html#country-based",
    "href": "posts/20250330_PhD-position/index.html#country-based",
    "title": "Resource to find a PhD (or researcher) position",
    "section": "Country-based",
    "text": "Country-based\n\n荷蘭：AcademicTransfer\n德國：jobvector\n英國：FindAPhD\n法國：PhD in France\n瑞典：varbi\n挪威：Jobbnorge"
  },
  {
    "objectID": "posts/20250328_VHS/index.html",
    "href": "posts/20250328_VHS/index.html",
    "title": "Brazilian Vaccine Attitude Analysis",
    "section": "",
    "text": "This article analyzes the attitudes of Brazilian healthcare professionals towards COVID-19 vaccination using the survey data. The results from this analysis will contribute to Hugo’s research paper. This analysis has two main objectives:"
  },
  {
    "objectID": "posts/20250328_VHS/index.html#prepare-variables",
    "href": "posts/20250328_VHS/index.html#prepare-variables",
    "title": "Brazilian Vaccine Attitude Analysis",
    "section": "2.1 Prepare variables",
    "text": "2.1 Prepare variables\nBefore conducting the factor analysis, 1715 variables related to general vaccine attitudes and specific COVID-19 attitudes were selected.\n\nfa_data &lt;- data |&gt;\n  select(matches(\"vac_gen_\\\\d\"), covid_pro_1:covid_oblig_2, covid_leaders:covid_earlyaccess) |&gt; \n  drop_na()\n\napply(fa_data, 2, table)\n\n  vac_gen_1 vac_gen_2 vac_gen_3 vac_gen_4 vac_gen_5 vac_gen_6 covid_pro_1\n1         7         5        93       125        29        48           8\n2        21         5       183       197        74        28          26\n4       143       107       199       167       253       111         126\n5       373       427        69        55       188       357         384\n  covid_ag_1 covid_ag_2 covid_pro_2 covid_over18 covid_pregnant covid_children\n1        370        296          10          352             28             22\n2         86        161          19          142             44             19\n4         45         54         118           34            188            142\n5         43         33         397           16            284            361\n  covid_oblig_1 covid_oblig_2 covid_leaders covid_earlyaccess\n1            32            65            20                10\n2            42            84            27                 9\n4           120           164           133               109\n5           350           231           364               416\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCaution that the numeric labels for the Likert scale responses (‘Strongly Disagree,’ ‘Disagree,’ ‘Agree,’ ‘Strongly Agree’) are coded as 1, 2, 4, and 5, respectively, rather than the more common sequential coding (e.g., 1, 2, 3, 4)\nThis non-sequential coding implies an assumption that the interval between ‘Disagree’ (2) and ‘Agree’ (4) is twice that of the other adjacent intervals. Treating these data as interval scale in the analysis relies on this assumption.\n\n\nTo according to the past literature, these variables were recoded from 1, 2, 4, and 5 to 1, 2, 3, and 4!\nIn addition, observations with missing values in any of these 17 variables were removed. Consequently, 544 complete observations remain for the factor analysis\n\nfa_data &lt;- fa_data |&gt; \n  mutate(across(everything(), ~ . - ifelse(. &gt; 3, 1, 0)))\n\ntable(fa_data$vac_gen_1) # check an example\n\n\n  1   2   3   4 \n  7  21 143 373 \n\n\nBefore performing the factor analysis, a correlation matrix is examined to visually inspect potential clustering among the attitude variables.\n\ncor_mat &lt;- cor(fa_data)\n\n# Copy from https://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram\n\ncor.mtest &lt;- function(mat, ...) {\n    mat &lt;- as.matrix(mat)\n    n &lt;- ncol(mat)\n    p.mat&lt;- matrix(NA, n, n)\n    diag(p.mat) &lt;- 0\n    for (i in 1:(n - 1)) {\n        for (j in (i + 1):n) {\n            tmp &lt;- cor.test(mat[, i], mat[, j], ...)\n            p.mat[i, j] &lt;- p.mat[j, i] &lt;- tmp$p.value\n        }\n    }\n  colnames(p.mat) &lt;- rownames(p.mat) &lt;- colnames(mat)\n  p.mat\n}\n# matrix of the p-value of the correlation\np.mat &lt;- cor.mtest(cor_mat)\n\ncorrplot(\n  cor_mat, method=\"color\", type=\"upper\", order=\"hclust\", \n  addCoef.col = \"black\", # Add coefficient of correlation\n  tl.col=\"black\", tl.srt=45, tl.cex=0.6, #Text label color and rotation\n  cl.cex = 0.5, number.cex = 0.75,\n  # Combine with significance\n  p.mat = p.mat, sig.level = 0.01, insig = \"blank\", \n  # hide correlation coefficient on the principal diagonal\n  diag=FALSE\n)\n\n\n\n\n\n\n\nFigure 1: The correlation matrix of attitudes variables\n\n\n\n\n\nThe correlation matrix suggests potential clustering into two or three groups:\n\nvac_gen_4, covid_ad_1, covid_ad_2, covid_over_18 form a cluster.\nOther variables (except for vac_gen_6) form a cluster.\nvac_gen_6 could might form its own cluster, but is closer to the second cluster.(However, subsequent analysis did not reveal distinct findings for this variable.)\n\nNote that this is a preliminary observation based on correlations. The underlying latent structure will be formally investigated using factor analysis."
  },
  {
    "objectID": "posts/20250328_VHS/index.html#factor-analysis",
    "href": "posts/20250328_VHS/index.html#factor-analysis",
    "title": "Brazilian Vaccine Attitude Analysis",
    "section": "2.2 Factor analysis",
    "text": "2.2 Factor analysis\n\n2.2.1 Pre-tests and scree plots\nKaiser-Meyer-Olkin (KMO) measure and Bartlett’s test of sphericity are used to assess the suitability of the data for factor analysis.\n\n\n\nKaiser-Meyer-Olkin (KMO) 測驗:\n\n意義: 評估樣本相關矩陣是否適合進行因子分析。\n評估: 值介於 0 到 1 之間，值越高表示適合度越高。通常認為大於 0.6 較佳。\n\nBartlett’s 檢驗:\n\n意義: 檢驗觀察變數之間的相關矩陣是否為一個單位矩陣 (即變數之間不相關)。\n評估: 顯著性水平 (p-value) 越小，表示變數之間存在顯著相關，適合進行因子分析。\n\n\n\n# KMO值檢驗（建議 &gt; 0.6）\nKMO_result &lt;- KMO(cor_mat)\nprint(KMO_result)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor_mat)\nOverall MSA =  0.94\nMSA for each item = \n        vac_gen_1         vac_gen_2         vac_gen_3         vac_gen_4 \n             0.94              0.94              0.63              0.69 \n        vac_gen_5         vac_gen_6       covid_pro_1        covid_ag_1 \n             0.97              0.94              0.97              0.96 \n       covid_ag_2       covid_pro_2      covid_over18    covid_pregnant \n             0.97              0.96              0.95              0.96 \n   covid_children     covid_oblig_1     covid_oblig_2     covid_leaders \n             0.94              0.96              0.96              0.96 \ncovid_earlyaccess \n             0.94 \n\n# Bartlett's 球形檢定 (需顯著p &lt; 0.05)\nbartlett_result &lt;- cortest.bartlett(cor_mat, n = nrow(data))\nprint(bartlett_result)\n\n$chisq\n[1] 6930.036\n\n$p.value\n[1] 0\n\n$df\n[1] 136\n\n\nBoth tests indicate that the data are suitable for factor analysis (KMO &gt; 0.6, Bartlett’s test p &lt; 0.05).\nA key challenge in EFA is determining the optimal number of factors to retain. Parallel analysis is employed to guide the decision on the number of factors. This procedure compares the eigenvalues from the actual data’s correlation matrix with eigenvalues derived from random data matrices of the same size. The number of factors suggested is the count of actual eigenvalues that exceed the corresponding eigenvalues (or the 95th percentile thereof) from the random data.\n\nfa.parallel(fa_data, fm = \"ml\", fa = \"fa\", nfactors = 1) \n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\n\n\n\n\n\n\nFigure 2: Scree plot of eigen values\n\n\n\n\n\nBased on the results (by the principal axis factor analysis), it suggests retaining 3 factors. However, this result is advisory, not definitive. On the one hand, if we use subjective method, e.g., the elbow method, might suggest that 2 factors are sufficient. On the other hand, if we use (not recommended) the rule of thumb (a.k.a. Kaiser criterion)–retaining component(s) with eigenvalues greater than 1.0–then, might suggest only one factor.\nThis analysis does not aim to develop a new measurement instrument or uncover ‘true’ latent constructs. Instead, the goal is to explore whether the factor structure in this data aligns with the established 5C model. To facilitate comparison with the 5C model, solutions with 3, 4, and 5 factors will be examined. Therefore, 3-, 4-, and 5-factor models are explored below. For each number of factors (3, 4, and 5), both orthogonal (“Varimax”) and oblique (“Oblimin”) rotations are applied. Factor scores are estimated using the regression method.\n\n\n因子解釋性指標 (Factor Interpretability Indices)\n這些指標評估因子是否具有清晰的解釋力，以及因子負載量是否合理。\n\n因子負載量 (Factor Loadings)：\n\n意義：衡量每個觀察變數在特定因子上的相關程度。\n評估：通常認為 0.3 或 0.4 以上的因子負載量具有實質意義。負載量越高，表示變數與因子之間的關係越強。\n\nh2:\n\nthe amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\n\nu2:\n\n1 - h2. residual variance, a.k.a. uniqueness\n\ncom:\n\nItem complexity. Specifically it is “Hoffman’s index of complexity for each item. This is just (_i2)2 / _i^4, where \\(\\lambda_i\\) is the factor loading on the ith factor. From Hofmann (1978), MBR. See also Pettersson and Turkheimer (2010).” It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc. Basically it tells you how much an item reflects a single construct. It will be lower for relatively lower loadings.\n\n共同性 (Communality)：\n\n意義：衡量每個觀察變數有多少變異量可以被因子解釋。\n評估：值越高，表示變數被因子解釋的程度越高。通常認為 0.4 或 0.5 以上較佳。\n\n因子負載矩陣的簡單結構 (Simple Structure)：\n\n意義：理想的因子矩陣應該具有簡單結構，即每個觀察變數在一個或少數幾個因子上有較高的負載量，而在其他因子上負載量接近於零。\n評估：觀察因子負載矩陣，看是否符合簡單結構的原則。\n\n因子可解釋變異量 (Percentage of Variance Explained)：\n\n意義：衡量因子解釋了觀察變數總變異量的百分比。\n評估：通常希望因子能解釋至少 50% 或 60% 的總變異量。\n\n\n\n\n2.2.2 Othogonal rotation\n\nn = 3n = 4n = 5\n\n\n\nfa3 &lt;- fa(fa_data, nfactors = 3, rotate = \"varimax\", scores = \"regression\", fm = \"ml\")\nfa3\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 3, rotate = \"varimax\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML1   ML3   ML2   h2   u2 com\nvac_gen_1          0.39  0.70 -0.11 0.66 0.34 1.6\nvac_gen_2          0.38  0.78 -0.09 0.77 0.23 1.5\nvac_gen_3         -0.10 -0.03  0.94 0.89 0.11 1.0\nvac_gen_4         -0.18 -0.07  0.82 0.71 0.29 1.1\nvac_gen_5          0.32  0.38 -0.10 0.26 0.74 2.1\nvac_gen_6          0.16  0.34  0.03 0.14 0.86 1.4\ncovid_pro_1        0.58  0.42 -0.07 0.52 0.48 1.9\ncovid_ag_1        -0.43 -0.30  0.22 0.32 0.68 2.3\ncovid_ag_2        -0.61 -0.42  0.18 0.58 0.42 2.0\ncovid_pro_2        0.63  0.53 -0.09 0.68 0.32 2.0\ncovid_over18      -0.67 -0.29  0.16 0.55 0.45 1.5\ncovid_pregnant     0.72  0.27 -0.15 0.61 0.39 1.4\ncovid_children     0.82  0.39 -0.11 0.83 0.17 1.5\ncovid_oblig_1      0.68  0.39 -0.08 0.62 0.38 1.6\ncovid_oblig_2      0.58  0.24 -0.13 0.41 0.59 1.4\ncovid_leaders      0.71  0.48 -0.08 0.74 0.26 1.8\ncovid_earlyaccess  0.55  0.59 -0.06 0.66 0.34 2.0\n\n                       ML1  ML3  ML2\nSS loadings           4.99 3.19 1.77\nProportion Var        0.29 0.19 0.10\nCumulative Var        0.29 0.48 0.59\nProportion Explained  0.50 0.32 0.18\nCumulative Proportion 0.50 0.82 1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 88  and the objective function was  0.6 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  116.72  with prob &lt;  0.022 \nThe total n.obs was  544  with Likelihood Chi Square =  318.07  with prob &lt;  8.9e-28 \n\nTucker Lewis Index of factoring reliability =  0.938\nRMSEA index =  0.069  and the 90 % confidence intervals are  0.061 0.078\nBIC =  -236.24\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2\nCorrelation of (regression) scores with factors   0.91 0.86 0.95\nMultiple R square of scores with factors          0.82 0.74 0.91\nMinimum correlation of possible factor scores     0.64 0.48 0.82\n\n\n\n\n\nfa4 &lt;- fa(fa_data, nfactors = 4, rotate = \"varimax\", scores = \"regression\", fm = \"ml\")\nfa4\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 4, rotate = \"varimax\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML2   ML3   ML1   ML4   h2    u2 com\nvac_gen_1          0.37  0.80 -0.11  0.08 0.80 0.197 1.5\nvac_gen_2          0.39  0.69 -0.09  0.28 0.72 0.281 2.0\nvac_gen_3         -0.09 -0.02  0.99 -0.03 1.00 0.005 1.0\nvac_gen_4         -0.19 -0.07  0.78 -0.04 0.64 0.355 1.1\nvac_gen_5          0.29  0.36 -0.10  0.18 0.26 0.741 2.7\nvac_gen_6          0.17  0.32  0.03  0.08 0.14 0.864 1.7\ncovid_pro_1        0.56  0.37 -0.07  0.26 0.52 0.483 2.3\ncovid_ag_1        -0.43 -0.26  0.21 -0.16 0.32 0.680 2.5\ncovid_ag_2        -0.61 -0.41  0.17 -0.16 0.59 0.408 2.1\ncovid_pro_2        0.57  0.41 -0.08  0.46 0.72 0.281 2.8\ncovid_over18      -0.70 -0.31  0.15 -0.02 0.61 0.387 1.5\ncovid_pregnant     0.71  0.24 -0.15  0.18 0.61 0.386 1.5\ncovid_children     0.81  0.35 -0.11  0.22 0.84 0.164 1.6\ncovid_oblig_1      0.65  0.34 -0.07  0.26 0.61 0.385 1.9\ncovid_oblig_2      0.57  0.23 -0.13  0.12 0.41 0.592 1.6\ncovid_leaders      0.66  0.38 -0.08  0.42 0.76 0.240 2.4\ncovid_earlyaccess  0.46  0.45 -0.06  0.63 0.81 0.191 2.7\n\n                       ML2  ML3  ML1  ML4\nSS loadings           4.69 2.69 1.79 1.19\nProportion Var        0.28 0.16 0.11 0.07\nCumulative Var        0.28 0.43 0.54 0.61\nProportion Explained  0.45 0.26 0.17 0.12\nCumulative Proportion 0.45 0.71 0.88 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 74  and the objective function was  0.37 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  84  with prob &lt;  0.2 \nThe total n.obs was  544  with Likelihood Chi Square =  198.21  with prob &lt;  2.8e-13 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.056  and the 90 % confidence intervals are  0.046 0.065\nBIC =  -267.91\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML2  ML3  ML1  ML4\nCorrelation of (regression) scores with factors   0.90 0.87 1.00 0.79\nMultiple R square of scores with factors          0.82 0.75 0.99 0.62\nMinimum correlation of possible factor scores     0.63 0.51 0.99 0.25\n\n\n\n\n\nfa5 &lt;- fa(fa_data, nfactors = 5, rotate = \"varimax\", scores = \"regression\", fm = \"ml\")\nfa5\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 5, rotate = \"varimax\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML3   ML2   ML5   ML1   ML4   h2    u2 com\nvac_gen_1          0.81  0.24  0.29 -0.11  0.10 0.81 0.187 1.5\nvac_gen_2          0.69  0.26  0.27 -0.09  0.31 0.72 0.285 2.1\nvac_gen_3         -0.03 -0.08 -0.04  0.99 -0.04 1.00 0.005 1.0\nvac_gen_4         -0.07 -0.14 -0.12  0.78 -0.05 0.65 0.354 1.1\nvac_gen_5          0.36  0.22  0.17 -0.11  0.20 0.26 0.741 3.0\nvac_gen_6          0.32  0.16  0.05  0.03  0.10 0.14 0.856 1.8\ncovid_pro_1        0.38  0.47  0.25 -0.07  0.31 0.53 0.470 3.4\ncovid_ag_1        -0.27 -0.35 -0.21  0.21 -0.20 0.32 0.677 4.0\ncovid_ag_2        -0.41 -0.46 -0.38  0.17 -0.21 0.59 0.409 3.7\ncovid_pro_2        0.42  0.41  0.34 -0.09  0.50 0.72 0.283 3.8\ncovid_over18      -0.34 -0.66 -0.26  0.15 -0.09 0.65 0.348 2.0\ncovid_pregnant     0.26  0.61  0.30 -0.15  0.26 0.62 0.375 2.4\ncovid_children     0.36  0.66  0.42 -0.11  0.29 0.84 0.164 2.9\ncovid_oblig_1      0.31  0.34  0.67 -0.08  0.26 0.73 0.265 2.4\ncovid_oblig_2      0.20  0.31  0.57 -0.14  0.12 0.50 0.503 2.1\ncovid_leaders      0.37  0.41  0.51 -0.09  0.45 0.78 0.222 3.8\ncovid_earlyaccess  0.46  0.29  0.27 -0.06  0.67 0.82 0.176 2.6\n\n                       ML3  ML2  ML5  ML1  ML4\nSS loadings           2.73 2.65 2.02 1.80 1.48\nProportion Var        0.16 0.16 0.12 0.11 0.09\nCumulative Var        0.16 0.32 0.44 0.54 0.63\nProportion Explained  0.26 0.25 0.19 0.17 0.14\nCumulative Proportion 0.26 0.50 0.69 0.86 1.00\n\nMean item complexity =  2.6\nTest of the hypothesis that 5 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 61  and the objective function was  0.24 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  53.91  with prob &lt;  0.73 \nThe total n.obs was  544  with Likelihood Chi Square =  125.5  with prob &lt;  2.3e-06 \n\nTucker Lewis Index of factoring reliability =  0.975\nRMSEA index =  0.044  and the 90 % confidence intervals are  0.033 0.055\nBIC =  -258.74\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML3  ML2  ML5  ML1  ML4\nCorrelation of (regression) scores with factors   0.87 0.82 0.77 1.00 0.81\nMultiple R square of scores with factors          0.76 0.68 0.59 0.99 0.66\nMinimum correlation of possible factor scores     0.53 0.35 0.18 0.99 0.32\n\n\n\n\n\n\nfa.diagram(fa3, sort = FALSE)\nfa.diagram(fa4, sort = FALSE)\nfa.diagram(fa5, sort = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(a) n = 3\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 4\n\n\n\n\n\n\n\n\n\n\n\n(c) n = 5\n\n\n\n\n\n\n\nFigure 3: Diagram of factor structure under the orthogonal rotation\n\n\n\n\n\n\n2.2.3 Oblique rotation\n\nn = 3n = 4n = 5\n\n\n\nfa3_ob &lt;- fa(fa_data, nfactors = 3, rotate = \"oblimin\", scores = \"regression\", fm = \"ml\")\nfa3_ob\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 3, rotate = \"oblimin\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML1   ML3   ML2   h2   u2 com\nvac_gen_1          0.07  0.75 -0.05 0.66 0.34 1.0\nvac_gen_2          0.00  0.87 -0.02 0.77 0.23 1.0\nvac_gen_3          0.04 -0.01  0.95 0.89 0.11 1.0\nvac_gen_4         -0.07 -0.01  0.82 0.71 0.29 1.0\nvac_gen_5          0.18  0.33 -0.05 0.26 0.74 1.6\nvac_gen_6          0.00  0.38  0.06 0.14 0.86 1.0\ncovid_pro_1        0.56  0.19  0.02 0.52 0.48 1.2\ncovid_ag_1        -0.39 -0.13  0.16 0.32 0.68 1.5\ncovid_ag_2        -0.59 -0.18  0.09 0.58 0.42 1.2\ncovid_pro_2        0.55  0.32  0.01 0.68 0.32 1.6\ncovid_over18      -0.77  0.06  0.05 0.55 0.45 1.0\ncovid_pregnant     0.86 -0.12 -0.04 0.61 0.39 1.0\ncovid_children     0.94 -0.03  0.02 0.83 0.17 1.0\ncovid_oblig_1      0.73  0.08  0.03 0.62 0.38 1.0\ncovid_oblig_2      0.68 -0.07 -0.04 0.41 0.59 1.0\ncovid_leaders      0.72  0.18  0.04 0.74 0.26 1.1\ncovid_earlyaccess  0.40  0.47  0.04 0.66 0.34 2.0\n\n                       ML1  ML3  ML2\nSS loadings           5.66 2.61 1.68\nProportion Var        0.33 0.15 0.10\nCumulative Var        0.33 0.49 0.59\nProportion Explained  0.57 0.26 0.17\nCumulative Proportion 0.57 0.83 1.00\n\n With factor correlations of \n      ML1   ML3   ML2\nML1  1.00  0.79 -0.28\nML3  0.79  1.00 -0.17\nML2 -0.28 -0.17  1.00\n\nMean item complexity =  1.2\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 88  and the objective function was  0.6 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  116.72  with prob &lt;  0.022 \nThe total n.obs was  544  with Likelihood Chi Square =  318.07  with prob &lt;  8.9e-28 \n\nTucker Lewis Index of factoring reliability =  0.938\nRMSEA index =  0.069  and the 90 % confidence intervals are  0.061 0.078\nBIC =  -236.24\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2\nCorrelation of (regression) scores with factors   0.97 0.94 0.96\nMultiple R square of scores with factors          0.95 0.89 0.92\nMinimum correlation of possible factor scores     0.89 0.78 0.83\n\n\n\n\n\nfa4_ob &lt;- fa(fa_data, nfactors = 4, rotate = \"oblimin\", scores = \"regression\", fm = \"ml\")\nfa4_ob\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 4, rotate = \"oblimin\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML2   ML3   ML1   ML4   h2    u2 com\nvac_gen_1          0.01  0.92 -0.01 -0.06 0.80 0.197 1.0\nvac_gen_2          0.03  0.68 -0.01  0.21 0.72 0.281 1.2\nvac_gen_3          0.05  0.01  1.01 -0.01 1.00 0.005 1.0\nvac_gen_4         -0.10  0.00  0.77  0.01 0.64 0.355 1.0\nvac_gen_5          0.13  0.30 -0.06  0.13 0.26 0.741 1.9\nvac_gen_6          0.03  0.33  0.07  0.04 0.14 0.864 1.1\ncovid_pro_1        0.49  0.15  0.02  0.16 0.52 0.483 1.4\ncovid_ag_1        -0.39 -0.09  0.15 -0.07 0.32 0.680 1.5\ncovid_ag_2        -0.57 -0.21  0.06 -0.02 0.59 0.408 1.3\ncovid_pro_2        0.40  0.13 -0.02  0.42 0.72 0.281 2.2\ncovid_over18      -0.83 -0.08  0.02  0.19 0.61 0.387 1.1\ncovid_pregnant     0.81 -0.09 -0.04  0.03 0.61 0.386 1.0\ncovid_children     0.89  0.00  0.02  0.05 0.84 0.164 1.0\ncovid_oblig_1      0.64  0.06  0.02  0.14 0.61 0.385 1.1\ncovid_oblig_2      0.64 -0.01 -0.04 -0.01 0.41 0.592 1.0\ncovid_leaders      0.57  0.06  0.00  0.34 0.76 0.240 1.7\ncovid_earlyaccess  0.14  0.20 -0.03  0.66 0.81 0.191 1.3\n\n                       ML2  ML3  ML1  ML4\nSS loadings           4.96 2.23 1.71 1.46\nProportion Var        0.29 0.13 0.10 0.09\nCumulative Var        0.29 0.42 0.52 0.61\nProportion Explained  0.48 0.22 0.17 0.14\nCumulative Proportion 0.48 0.69 0.86 1.00\n\n With factor correlations of \n      ML2   ML3   ML1   ML4\nML2  1.00  0.75 -0.29  0.63\nML3  0.75  1.00 -0.21  0.58\nML1 -0.29 -0.21  1.00 -0.08\nML4  0.63  0.58 -0.08  1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 74  and the objective function was  0.37 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  84  with prob &lt;  0.2 \nThe total n.obs was  544  with Likelihood Chi Square =  198.21  with prob &lt;  2.8e-13 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.056  and the 90 % confidence intervals are  0.046 0.065\nBIC =  -267.91\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML2  ML3  ML1  ML4\nCorrelation of (regression) scores with factors   0.97 0.94 1.00 0.90\nMultiple R square of scores with factors          0.94 0.89 1.00 0.81\nMinimum correlation of possible factor scores     0.88 0.78 0.99 0.62\n\n\n\n\n\nfa5_ob &lt;- fa(fa_data, nfactors = 5, rotate = \"oblimin\", scores = \"regression\", fm = \"ml\")\nfa5_ob\n\nFactor Analysis using method =  ml\nCall: fa(r = fa_data, nfactors = 5, rotate = \"oblimin\", scores = \"regression\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML2   ML3   ML5   ML4   ML1   h2    u2 com\nvac_gen_1          0.01  0.89  0.04 -0.04 -0.01 0.81 0.187 1.0\nvac_gen_2          0.04  0.62  0.02  0.25 -0.01 0.72 0.285 1.3\nvac_gen_3          0.02  0.01  0.04 -0.01  1.01 1.00 0.005 1.0\nvac_gen_4         -0.04 -0.01 -0.07  0.02  0.77 0.65 0.354 1.0\nvac_gen_5          0.12  0.27  0.01  0.16 -0.06 0.26 0.741 2.2\nvac_gen_6          0.15  0.30 -0.13  0.08  0.08 0.14 0.856 2.3\ncovid_pro_1        0.46  0.12  0.00  0.24  0.02 0.53 0.470 1.7\ncovid_ag_1        -0.33 -0.08 -0.04 -0.12  0.15 0.32 0.677 1.8\ncovid_ag_2        -0.37 -0.21 -0.21 -0.05  0.07 0.59 0.409 2.3\ncovid_pro_2        0.25  0.09  0.14  0.47 -0.02 0.72 0.283 1.8\ncovid_over18      -0.84 -0.09  0.04  0.11  0.02 0.65 0.348 1.1\ncovid_pregnant     0.70 -0.08  0.06  0.12 -0.05 0.62 0.375 1.1\ncovid_children     0.67  0.01  0.19  0.12  0.01 0.84 0.164 1.2\ncovid_oblig_1      0.00  0.07  0.78  0.06  0.01 0.73 0.265 1.0\ncovid_oblig_2      0.06  0.02  0.68 -0.10 -0.05 0.50 0.503 1.1\ncovid_leaders      0.16  0.04  0.44  0.35  0.00 0.78 0.222 2.2\ncovid_earlyaccess  0.04  0.12  0.06  0.75 -0.03 0.82 0.176 1.1\n\n                       ML2  ML3  ML5  ML4  ML1\nSS loadings           3.09 2.02 2.00 1.85 1.72\nProportion Var        0.18 0.12 0.12 0.11 0.10\nCumulative Var        0.18 0.30 0.42 0.53 0.63\nProportion Explained  0.29 0.19 0.19 0.17 0.16\nCumulative Proportion 0.29 0.48 0.66 0.84 1.00\n\n With factor correlations of \n      ML2   ML3   ML5   ML4   ML1\nML2  1.00  0.69  0.80  0.63 -0.29\nML3  0.69  1.00  0.65  0.63 -0.20\nML5  0.80  0.65  1.00  0.64 -0.23\nML4  0.63  0.63  0.64  1.00 -0.11\nML1 -0.29 -0.20 -0.23 -0.11  1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 5 factors are sufficient.\n\ndf null model =  136  with the objective function =  11.06 with Chi Square =  5934.5\ndf of  the model are 61  and the objective function was  0.24 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  544 with the empirical chi square  53.91  with prob &lt;  0.73 \nThe total n.obs was  544  with Likelihood Chi Square =  125.5  with prob &lt;  2.3e-06 \n\nTucker Lewis Index of factoring reliability =  0.975\nRMSEA index =  0.044  and the 90 % confidence intervals are  0.033 0.055\nBIC =  -258.74\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML2  ML3  ML5  ML4  ML1\nCorrelation of (regression) scores with factors   0.95 0.94 0.93 0.92 1.00\nMultiple R square of scores with factors          0.91 0.88 0.87 0.86 0.99\nMinimum correlation of possible factor scores     0.81 0.76 0.74 0.71 0.99\n\n\n\n\n\nI prefer the fa5_ob model. However, caution is advised regarding the item complexity (com) values which are larger than 2. This suggests these items load substantially on more than one factor, indicating potential cross-loading. Similar cross-loading issues are observed in other solutions as well.\n\nfa.diagram(fa3_ob, sort = FALSE)\nfa.diagram(fa4_ob, sort = FALSE)\nfa.diagram(fa5_ob, sort = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(a) n = 3\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 4\n\n\n\n\n\n\n\n\n\n\n\n(c) n = 5\n\n\n\n\n\n\n\nFigure 4: Diagram of factor structure under the oblique rotation\n\n\n\n\n\n\n2.2.4 Overall comparison\n\nassign_fac &lt;- function(f){\n  fac_load_abs &lt;- abs(f$loadings)\n  mac_loadings &lt;- apply(fac_load_abs, 1, which.max)\n}\n\nfa_models &lt;- list(fa3, fa4, fa5, fa3_ob, fa4_ob, fa5_ob)\nfa_load_comp &lt;- map_dfc(fa_models, assign_fac)\nnames(fa_load_comp) &lt;- c(\"fa3\", \"fa4\", \"fa5\", \"fa3_ob\", \"fa4_ob\", \"fa5_ob\")\n\nfa_load_comp &lt;- fa_load_comp |&gt; \n  add_column(var = colnames(fa_data), .before = \"fa3\")\nfa_load_comp\n\n# A tibble: 17 × 7\n   var                 fa3   fa4   fa5 fa3_ob fa4_ob fa5_ob\n   &lt;chr&gt;             &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 vac_gen_1             2     2     1      2      2      2\n 2 vac_gen_2             2     2     1      2      2      2\n 3 vac_gen_3             3     3     4      3      3      5\n 4 vac_gen_4             3     3     4      3      3      5\n 5 vac_gen_5             2     2     1      2      2      2\n 6 vac_gen_6             2     2     1      2      2      2\n 7 covid_pro_1           1     1     2      1      1      1\n 8 covid_ag_1            1     1     2      1      1      1\n 9 covid_ag_2            1     1     2      1      1      1\n10 covid_pro_2           1     1     5      1      4      4\n11 covid_over18          1     1     2      1      1      1\n12 covid_pregnant        1     1     2      1      1      1\n13 covid_children        1     1     2      1      1      1\n14 covid_oblig_1         1     1     3      1      1      3\n15 covid_oblig_2         1     1     3      1      1      3\n16 covid_leaders         1     1     3      1      1      3\n17 covid_earlyaccess     2     4     5      2      4      4\n\n\n\n\n\n\n\n\nFigure 5: FA structure comparison\n\n\n\nComparing the factor structures visually (Figure Figure 5), the 5-factor oblique solution (fa5_ob) seems most readily interpretable in the context of the 5C model.\nHowever, model fit indices are needed to quantitatively evaluate the performance of each model. Commonly reported fit indices are presented below (Table Table 1), with explanations provided in the adjacent notes.\n\n\n\nTable 1: Fitting indices of the models\n\n\nfa_model_fit &lt;- tibble(models = fa_models, \n                       names = c(\"fa3\", \"fa4\", \"fa5\", \"fa3_ob\", \"fa4_ob\", \"fa5_ob\")) |&gt; \n  mutate(fit = map_dbl(models, `$`, fit),\n         fit.off = map_dbl(models, `$`, fit.off),\n         chi2 = map_dbl(models, `$`, STATISTIC), # chi-square\n         df =  map_dbl(models, `$`, dof), # degree of freedom\n         p.value = map_dbl(models, `$`, PVAL), # p-value of the chi-square test\n         RMS = map_dbl(models, `$`, rms), # root mean square (off diagonal residuals) / df\n         cRMS = map_dbl(models, `$`, crms), # rms adjusted for degrees of freedom\n         RMSEA = map_dbl(models, ~ .$RMSEA[[1]]), # root mean square error of approximation\n         TLI = map_dbl(models, `$`, TLI), # Tucker-Lewis Index (the non-normal fit index)\n         BIC = map_dbl(models, `$`, BIC)) |&gt; \n  select(-models)\n\n#map_dbl(fa_models, `$`, chi) # emperical chi-square\n#map_dbl(fa_models, `$`, objective) # objectibe function used in ML\n#map_dbl(fa_models, `$`, eBIC) # emperical BIC\n\nfa_model_fit\n\n# A tibble: 6 × 11\n  names    fit fit.off  chi2    df  p.value    RMS   cRMS  RMSEA   TLI   BIC\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 fa3    0.951   0.996  318.    88 8.93e-28 0.0281 0.0349 0.0693 0.938 -236.\n2 fa4    0.954   0.997  198.    74 2.76e-13 0.0238 0.0323 0.0555 0.960 -268.\n3 fa5    0.958   0.998  126.    61 2.29e- 6 0.0191 0.0285 0.0440 0.975 -259.\n4 fa3_ob 0.951   0.996  318.    88 8.93e-28 0.0281 0.0349 0.0693 0.938 -236.\n5 fa4_ob 0.954   0.997  198.    74 2.76e-13 0.0238 0.0323 0.0555 0.960 -268.\n6 fa5_ob 0.958   0.998  126.    61 2.29e- 6 0.0191 0.0285 0.0440 0.975 -259.\n\n\n\n\n\n\n模型擬合度指標 (Model Fit Indices)\n這些指標評估模型是否能很好地再現觀察變數之間的相關性。通常，我們會同時考量多個指標，而非僅僅依賴單一指標。\n\n卡方統計量 (Chi-Square Statistic, χ²)：\n\n意義：檢驗觀察相關矩陣與模型預測相關矩陣之間的差異。\n評估：值越小，表示模型擬合越好。但卡方統計量對樣本大小非常敏感，樣本越大，越容易顯著。\n限制：通常不單獨使用，需要結合其他指標。\n\n卡方自由度比 (Chi-Square/Degrees of Freedom, χ²/df)：\n\n意義：將卡方統計量除以自由度，以校正樣本大小的影響。\n評估：通常認為小於 3 較佳，小於 5 可以接受。\n\n比較適配指數 (Comparative Fit Index, CFI)：\n\n意義：比較目標模型與零模型 (所有變數不相關) 的適配程度。\n評估：值越接近 1，表示模型擬合越好。通常認為大於 0.90 或 0.95 較佳。\n\n標準化根均方誤差 (Root Mean Square Error of Approximation, RMSEA)：\n\n意義：衡量模型與真實數據之間的平均誤差。\n評估：值越小，表示模型擬合越好。通常認為小於 0.08 或 0.06 較佳。\n\nTucker-Lewis Index (TLI)：\n\n意義：類似 CFI，比較目標模型與零模型，但對模型複雜度有懲罰。\n評估：值越接近 1，表示模型擬合越好。通常認為大於 0.90 或 0.95 較佳。\n\nGoodness-of-Fit Index (GFI) 和 Adjusted Goodness-of-Fit Index (AGFI)：\n\n意義：衡量模型與觀察相關矩陣的整體適配程度。AGFI 會考慮模型複雜度。\n評估：值越接近 1，表示模型擬合越好。通常認為大於 0.90 較佳。\n\n\n實務意義指標 (Practical Significance)\n除了統計指標外，還需要考量模型是否具有實務意義，以及因子是否能提供有用的洞見。\n\n因子命名 (Factor Naming)：\n\n意義：根據因子負載量，為每個因子賦予一個有意義的名稱。\n評估：因子名稱應該能反映因子所代表的潛在概念。\n\n理論支持 (Theoretical Support)：\n\n意義：模型中的因子應該與現有的理論或研究結果相符。\n評估：檢查因子是否與相關文獻中的概念一致。\n\n模型簡潔性 (Parsimony)：\n\n意義：在解釋力相似的情況下，選擇較簡潔的模型。\n評估：避免過度複雜的模型，因為它們可能難以解釋和應用。\n\n\nSee also:\n\nPreacher & MacCallum (2003)\nFactor Analysis with the psych package: Making sense of the results\nHow To: Use the psych package for Factor Analysis and data reduction\n\n\n\n2.2.5 Final decision\nMost model fit indices suggest that all explored models provide a reasonably adequate fit to the data (e.g., TLI &gt; 0.90, RMSEA &lt; 0.08). Althought parallel analysis suggested 3 factors. But based on the BIC, it would prefer fa4 or fa4_ob models.\nHowever, considering the theoretical goal of relating the findings to the 5C model of vaccine hesitancy, the 5-factor oblique solution (i.e., fa5_ob) is selected as the final model. This choice is motivated by its clearer potential alignment with the theoretical framework and the fact that each factor in this solution has substantial loadings from multiple variables. Furthermore, while not having the absolute lowest BIC, it demonstrates strong performance across other fit indices (e.g., TLI, RMSEA).\nIt is recommended to report at least the factor loadings for the fa5_ob solution (Table Table 2), potentially using a cutoff (= 0.26) for clarity …\n\n#Should be report:  factor loadings\nfa_load &lt;- fa5_ob$loadings[,] %&gt;%\n  ifelse(abs(.) &gt; 0.26, ., NA) |&gt; # putting cutoff at 0.26\n  round(3) |&gt; \n  as_tibble() |&gt; \n  add_column(vars = names(fa_data), .before = 1)\n\ngt(fa_load) |&gt; \n  sub_missing(missing_text = \"-\") \n\n\n\nTable 2: The factor loadings\n\n\n\n\n\n\n\n\n\nvars\nML2\nML3\nML5\nML4\nML1\n\n\n\n\nvac_gen_1\n-\n0.891\n-\n-\n-\n\n\nvac_gen_2\n-\n0.620\n-\n-\n-\n\n\nvac_gen_3\n-\n-\n-\n-\n1.011\n\n\nvac_gen_4\n-\n-\n-\n-\n0.771\n\n\nvac_gen_5\n-\n0.269\n-\n-\n-\n\n\nvac_gen_6\n-\n0.299\n-\n-\n-\n\n\ncovid_pro_1\n0.459\n-\n-\n-\n-\n\n\ncovid_ag_1\n-0.333\n-\n-\n-\n-\n\n\ncovid_ag_2\n-0.371\n-\n-\n-\n-\n\n\ncovid_pro_2\n-\n-\n-\n0.470\n-\n\n\ncovid_over18\n-0.841\n-\n-\n-\n-\n\n\ncovid_pregnant\n0.700\n-\n-\n-\n-\n\n\ncovid_children\n0.669\n-\n-\n-\n-\n\n\ncovid_oblig_1\n-\n-\n0.777\n-\n-\n\n\ncovid_oblig_2\n-\n-\n0.684\n-\n-\n\n\ncovid_leaders\n-\n-\n0.445\n0.350\n-\n\n\ncovid_earlyaccess\n-\n-\n-\n0.752\n-\n\n\n\n\n\n\n\n\n\n\nand the inter-factor correlation matrix (Phi matrix, Table Table 3), as an oblique rotation was used, allowing factors to be correlated.\n\nfa_phi &lt;- fa5_ob$Phi |&gt; \n  round(3) |&gt; \n  as_tibble() |&gt; \n  add_column(\"_\"= colnames(fa5_ob$Phi), .before = 1)\n\ngt(fa_phi)\n\n\n\nTable 3: The correlation matrix of inter factors\n\n\n\n\n\n\n\n\n\n_\nML2\nML3\nML5\nML4\nML1\n\n\n\n\nML2\n1.000\n0.688\n0.798\n0.632\n-0.291\n\n\nML3\n0.688\n1.000\n0.648\n0.627\n-0.199\n\n\nML5\n0.798\n0.648\n1.000\n0.642\n-0.235\n\n\nML4\n0.632\n0.627\n0.642\n1.000\n-0.114\n\n\nML1\n-0.291\n-0.199\n-0.235\n-0.114\n1.000\n\n\n\n\n\n\n\n\n\n\n\n# be used in latter analysis\nfa_scores &lt;- fa5_ob$scores |&gt; as_tibble() \n\n\n\n\n\n\n\nNote\n\n\n\nStandard reporting guidelines for factor analysis from relevant literature should be consulted to determine the full set of results to present.\n\n\n\n\n\n\n\n\nWhy use a EFA instead of a CFA?\n\n\n\nAn EFA was chosen over a Confirmatory Factor Analysis (CFA) because the original questionnaire was not specifically designed a priori to measure the 5C constructs. EFA allows for exploration of the underlying factor structure without the strong theoretical constraints imposed by CFA."
  },
  {
    "objectID": "posts/20250328_VHS/index.html#define-variables",
    "href": "posts/20250328_VHS/index.html#define-variables",
    "title": "Brazilian Vaccine Attitude Analysis",
    "section": "3.1 Define variables",
    "text": "3.1 Define variables\n\nsentiment_var &lt;- c(\"cov_pro_3\", \"change_vac_1\", \"change_vac_2\", \"change_vac_3\", \"change_HealthSy_1\")\ndemography_var &lt;- c(\"gender\", \"age_class\", \"edu\", \"chronic\")\nworking_var &lt;- c(\"member\", \"prof\", \"w_setting\")\nfn_var &lt;- c(\"fear\", \"knowledge\")\nattitude_var &lt;- names(fa_data)\nfa_var &lt;- names(fa_scores)\n\nlr_rawdata &lt;- rawdata |&gt; \n  drop_na(all_of(attitude_var)) |&gt; \n  select(\n    all_of(sentiment_var), \n    all_of(demography_var), \n    all_of(working_var),\n    all_of(fn_var),\n    all_of(attitude_var)) |&gt; \n  add_column(fa_scores)\n\nThe list of the variables:\n\nOutcome variables:\n\nSentiment variables (S1-S5): were recoded into binary factors (0 = Strongly Disagree/Disagree, 1 = Agree/Strongly Agree).”\n\nPredictor variables:\n\nGender (gender): Recoded into a binary factor\n\nFemale and Male\n(excluding ‘I prefer not to answer’ which seen as a missing value)\n\nAge Class (age_class): Treated as a categorical factor with 6 levels\n\n&lt;35, 35-44, 45-54, 55-64, 65-74, and 75+\n\nEducation (edu): Treated as a binary factor\n\nMedical Degree and Other (collapsing Other, High School, Technical, Bachelor, Master)\n\nChronic Diseace (chronic): Recoded into a binary factor\n\nYes and No\n(excluding ‘I prefer not to answer’ which seen as a missing value)\n\nMember (member): Recoded into a binary factor\n\nYes and No\n(excluding ‘I don’t know’ which seen as a missing value)\n\nProfession (prof): Treated as a categorical factor with 5 levels\n\nNurs, Nutr, Phar, Phys and Other (collapsing Othe, Dent, Heal, Midf, Publ, Soci)\n\nWork setting (w_setting): Treated as a categorical factor with 5 levels\n\nComm, Hosp, Priv (collapsing private company and private practice), Univ, and Other\n\nFear (fear): Treated as continuous predictors.\nKnowledge (knowledge): Treated as continuous predictors.”\n5C Factors (ML1-ML5): The factor scores derived from the fa5_ob model were used as continuous predictors.”\n\n\nSimilarly, observations with missing values in any of these 17 variables were removed. Total 544 complete observations remain for the logistic regression.\n\nlr_data &lt;- lr_rawdata |&gt; \n  mutate(\n    # for sentiment variables\n    across(all_of(sentiment_var), ~ factor(ifelse(. &gt; 3, 1, 0))),\n    # for demography variables\n    gender = as_factor(gender) |&gt; droplevels(exclude = \"I prefer not to answer\"),\n    age_class = as_factor(age_class),\n    edu = as_factor(edu) |&gt; fct_relabel(~ str_sub(.x, 1, 4)) |&gt; fct_collapse(Othe = c(\"Othe\", \"High\", \"Tech\", \"Bach\", \"Mast\")),\n    chronic = as_factor(chronic) |&gt; droplevels(exclude = \"I prefer not to answer\"),\n    # for working variables\n    member = as_factor(member) |&gt; droplevels(exclude = \"I don't know\"),\n    prof = as_factor(prof) |&gt; fct_relabel(~ str_sub(.x, 1, 4)) |&gt; fct_collapse(Othe = c(\"Othe\", \"Dent\", \"Heal\", \"Midw\", \"Publ\", \"Soci\")),\n    w_setting = as_factor(w_setting) |&gt; fct_relabel(~ str_sub(.x, 1, 4)),\n    # for attitude variables: change labeling from 1245 to 1234\n    across(all_of(attitude_var), ~ . - ifelse(. &gt; 3, 1, 0))\n  )"
  },
  {
    "objectID": "posts/20250328_VHS/index.html#logistic-regression-analysis",
    "href": "posts/20250328_VHS/index.html#logistic-regression-analysis",
    "title": "Brazilian Vaccine Attitude Analysis",
    "section": "3.2 Logistic regression analysis",
    "text": "3.2 Logistic regression analysis\n\n3.2.1 Sentiment 1 (S1)\nThe formulas for the five hierarchical models for Sentiment 1 (change_vac_1) are defined as follows:\n\nm1_str &lt;- str_c(\"change_vac_1 ~ \", str_flatten(demography_var, \" + \"))\nm2_str &lt;- str_c(m1_str, \" + \", str_flatten(working_var, \" + \"))\nm3_str &lt;- str_c(m2_str, \" + \", \"fear\")\nm4_str &lt;- str_c(m3_str, \" + \", \"knowledge\")\nm5_str &lt;- str_c(m4_str, \" + \", str_flatten(fa_var, \" + \"))\n\nEach model is estimated using logistic regression (binomial family with logit link function).\n\ns1_models &lt;- map(list(m1_str, m2_str, m3_str, m4_str, m5_str), \n                 ~ glm(as.formula(.), family = binomial, lr_data)) \n\nLogistic regression results for S1 are presented in Table Table 5.\n\nextract_esti &lt;- function(lr) {\n  coef &lt;- summary(lr)$coefficients\n  var_name &lt;- rownames(coef)\n  esti &lt;- round(coef[, 1], 3)\n  p_value &lt;- coef[, 4]\n  conf_l &lt;- round(confint(lr)[, 1], 3)\n  conf_u &lt;- round(confint(lr)[, 2], 3)\n  p_sig &lt;- cut(p_value, \n               breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n               labels = c(\"***&nbsp;\", \"**&nbsp;&nbsp;\", \"*&nbsp;&nbsp;&nbsp;\", \"&nbsp;&nbsp;&nbsp;&nbsp;\", \"&nbsp;&nbsp;&nbsp;&nbsp;\"))\n  result &lt;- str_glue(\"{format(esti)}{p_sig}&lt;br&gt;[{format(conf_l)}, {format(conf_u)}]\")\n  tibble(vars = var_name, esti = result)\n}\n\ns1_result_comp &lt;- map(s1_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(s1_result_comp) &lt;- c(\"EVs\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\")\n\ngt(s1_result_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 5: Sentiment 1 result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n-0.006    [-0.608, 0.601]\n-0.377    [-1.395, 0.648]\n-0.391    [-1.648, 0.875]\n-1.092    [-2.627, 0.441]\n-0.793    [-2.394, 0.815]\n\n\ngenderMale\n0.160    [-0.309, 0.648]\n0.127    [-0.361, 0.632]\n0.127    [-0.362, 0.634]\n0.089    [-0.406, 0.599]\n0.123    [-0.379, 0.641]\n\n\nage_class35-44\n0.480    [-0.079, 1.044]\n0.662*   [ 0.068, 1.263]\n0.662*   [ 0.068, 1.264]\n0.631*   [ 0.034, 1.235]\n0.698*   [ 0.091, 1.314]\n\n\nage_class45-54\n0.442    [-0.184, 1.081]\n0.611    [-0.068, 1.306]\n0.611    [-0.067, 1.307]\n0.613    [-0.067, 1.310]\n0.727*   [ 0.030, 1.443]\n\n\nage_class55-64\n1.095**  [ 0.443, 1.771]\n1.114**  [ 0.392, 1.862]\n1.115**  [ 0.392, 1.864]\n1.103**  [ 0.378, 1.853]\n1.190**  [ 0.452, 1.954]\n\n\nage_class65-74\n2.341*** [ 1.187, 3.841]\n2.314*** [ 1.102, 3.851]\n2.314*** [ 1.102, 3.851]\n2.274*** [ 1.063, 3.809]\n2.361*** [ 1.139, 3.904]\n\n\nage_class75+\n-0.501    [-2.212, 1.113]\n-0.694    [-2.464, 0.983]\n-0.694    [-2.464, 0.984]\n-0.725    [-2.507, 0.962]\n-0.607    [-2.412, 1.111]\n\n\neduMedi\n-0.004    [-0.465, 0.460]\n-0.180    [-0.714, 0.355]\n-0.179    [-0.714, 0.356]\n-0.202    [-0.741, 0.337]\n-0.175    [-0.723, 0.372]\n\n\nchronicNo\n0.606*   [ 0.116, 1.092]\n0.590*   [ 0.081, 1.094]\n0.591*   [ 0.080, 1.098]\n0.576*   [ 0.063, 1.085]\n0.579*   [ 0.059, 1.094]\n\n\nmemberNo\n-\n0.104    [-0.540, 0.742]\n0.105    [-0.541, 0.745]\n0.111    [-0.536, 0.753]\n0.153    [-0.505, 0.807]\n\n\nprofNurs\n-\n-0.261    [-1.518, 1.061]\n-0.260    [-1.517, 1.062]\n-0.214    [-1.469, 1.106]\n-0.115    [-1.397, 1.226]\n\n\nprofNutr\n-\n0.180    [-0.397, 0.760]\n0.180    [-0.398, 0.762]\n0.184    [-0.398, 0.768]\n0.256    [-0.338, 0.854]\n\n\nprofPhar\n-\n0.462    [-0.261, 1.209]\n0.462    [-0.262, 1.209]\n0.463    [-0.263, 1.215]\n0.393    [-0.348, 1.157]\n\n\nprofPhys\n-\n0.609    [-0.208, 1.529]\n0.609    [-0.209, 1.529]\n0.596    [-0.222, 1.516]\n0.558    [-0.266, 1.483]\n\n\nw_settingComm\n-\n0.307    [-0.460, 1.079]\n0.307    [-0.460, 1.080]\n0.292    [-0.476, 1.065]\n0.358    [-0.420, 1.142]\n\n\nw_settingHosp\n-\n0.260    [-0.530, 1.057]\n0.261    [-0.531, 1.061]\n0.276    [-0.519, 1.081]\n0.402    [-0.408, 1.225]\n\n\nw_settingPriv\n-\n-0.285    [-1.014, 0.427]\n-0.285    [-1.014, 0.427]\n-0.251    [-0.983, 0.465]\n-0.152    [-0.898, 0.580]\n\n\nw_settingUniv\n-\n0.340    [-0.431, 1.108]\n0.340    [-0.431, 1.109]\n0.317    [-0.456, 1.087]\n0.325    [-0.455, 1.104]\n\n\nfear\n-\n-\n0.001    [-0.076, 0.077]\n-0.001    [-0.079, 0.075]\n-0.043    [-0.133, 0.043]\n\n\nknowledge\n-\n-\n-\n0.105    [-0.025, 0.236]\n0.086    [-0.050, 0.223]\n\n\nML2\n-\n-\n-\n-\n0.107    [-0.462, 0.671]\n\n\nML3\n-\n-\n-\n-\n-0.074    [-0.480, 0.321]\n\n\nML5\n-\n-\n-\n-\n0.387    [-0.149, 0.922]\n\n\nML4\n-\n-\n-\n-\n-0.131    [-0.549, 0.281]\n\n\nML1\n-\n-\n-\n-\n0.020    [-0.219, 0.260]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Sentiment 2 (S2)\n\ns2_models &lt;- map(s1_models, \n                 ~ update(.x, change_vac_2 ~ .))\n\ns2_result_comp &lt;- map(s2_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(s2_result_comp) &lt;- c(\"EVs\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\")\n\ngt(s2_result_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 6: Sentiment 2 result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n1.308*** [ 0.656, 1.997]\n2.041*** [ 0.942, 3.205]\n0.173    [-1.152, 1.526]\n-0.483    [-2.094, 1.146]\n0.388    [-1.398, 2.212]\n\n\ngenderMale\n-0.090    [-0.563, 0.401]\n-0.007    [-0.498, 0.501]\n0.087    [-0.422, 0.614]\n0.047    [-0.466, 0.577]\n0.081    [-0.492, 0.676]\n\n\nage_class35-44\n-0.039    [-0.634, 0.550]\n-0.154    [-0.784, 0.466]\n-0.100    [-0.757, 0.548]\n-0.124    [-0.782, 0.525]\n0.050    [-0.692, 0.787]\n\n\nage_class45-54\n0.150    [-0.524, 0.834]\n-0.135    [-0.857, 0.592]\n-0.120    [-0.870, 0.633]\n-0.114    [-0.863, 0.637]\n0.224    [-0.611, 1.067]\n\n\nage_class55-64\n0.058    [-0.590, 0.706]\n-0.378    [-1.105, 0.341]\n-0.294    [-1.050, 0.454]\n-0.301    [-1.058, 0.449]\n-0.117    [-0.950, 0.708]\n\n\nage_class65-74\n2.723**  [ 1.086, 5.634]\n2.226*   [ 0.544, 5.150]\n2.325*   [ 0.605, 5.264]\n2.306*   [ 0.587, 5.244]\n2.435*   [ 0.656, 5.408]\n\n\nage_class75+\n-0.195    [-1.844, 1.821]\n-0.589    [-2.305, 1.469]\n-0.461    [-2.287, 1.707]\n-0.469    [-2.311, 1.706]\n0.392    [-1.718, 2.885]\n\n\neduMedi\n-0.156    [-0.618, 0.310]\n-0.419    [-0.963, 0.123]\n-0.369    [-0.940, 0.199]\n-0.383    [-0.956, 0.188]\n-0.384    [-1.015, 0.243]\n\n\nchronicNo\n-0.201    [-0.748, 0.317]\n-0.184    [-0.743, 0.349]\n-0.042    [-0.619, 0.511]\n-0.059    [-0.638, 0.496]\n-0.077    [-0.712, 0.529]\n\n\nmemberNo\n-\n-0.037    [-0.715, 0.630]\n0.104    [-0.610, 0.810]\n0.101    [-0.615, 0.809]\n0.297    [-0.462, 1.053]\n\n\nprofNurs\n-\n0.285    [-1.155, 2.213]\n0.416    [-1.082, 2.389]\n0.466    [-1.031, 2.436]\n0.762    [-0.889, 2.899]\n\n\nprofNutr\n-\n-0.354    [-0.948, 0.232]\n-0.288    [-0.905, 0.323]\n-0.274    [-0.893, 0.339]\n-0.169    [-0.875, 0.531]\n\n\nprofPhar\n-\n0.649    [-0.090, 1.420]\n0.590    [-0.171, 1.382]\n0.596    [-0.168, 1.393]\n0.318    [-0.515, 1.178]\n\n\nprofPhys\n-\n0.132    [-0.645, 0.975]\n0.108    [-0.700, 0.981]\n0.098    [-0.710, 0.972]\n0.007    [-0.854, 0.934]\n\n\nw_settingComm\n-\n-0.648    [-1.462, 0.139]\n-0.601    [-1.441, 0.213]\n-0.617    [-1.460, 0.199]\n-0.494    [-1.409, 0.398]\n\n\nw_settingHosp\n-\n-0.743    [-1.568, 0.057]\n-0.642    [-1.499, 0.195]\n-0.640    [-1.501, 0.199]\n-0.346    [-1.304, 0.603]\n\n\nw_settingPriv\n-\n-0.564    [-1.362, 0.201]\n-0.551    [-1.375, 0.241]\n-0.532    [-1.359, 0.263]\n-0.059    [-0.960, 0.826]\n\n\nw_settingUniv\n-\n-0.258    [-1.104, 0.564]\n-0.267    [-1.156, 0.598]\n-0.305    [-1.198, 0.563]\n-0.473    [-1.418, 0.440]\n\n\nfear\n-\n-\n0.200*** [ 0.127, 0.274]\n0.199*** [ 0.126, 0.273]\n0.095*   [ 0.001, 0.187]\n\n\nknowledge\n-\n-\n-\n0.098    [-0.039, 0.235]\n0.046    [-0.110, 0.200]\n\n\nML2\n-\n-\n-\n-\n0.081    [-0.550, 0.706]\n\n\nML3\n-\n-\n-\n-\n0.571*   [ 0.125, 1.025]\n\n\nML5\n-\n-\n-\n-\n0.719*   [ 0.140, 1.306]\n\n\nML4\n-\n-\n-\n-\n-0.125    [-0.587, 0.330]\n\n\nML1\n-\n-\n-\n-\n0.289    [ 0.000, 0.588]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Sentiment 3 (S3)\n\ns3_models &lt;- map(s1_models, \n                 ~ update(.x, change_HealthSy_1 ~ .))\n\ns3_result_comp &lt;- map(s3_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(s3_result_comp) &lt;- c(\"EVs\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\")\n\ngt(s3_result_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 7: Sentiment 3 result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n0.685*   [ 0.088, 1.300]\n2.008*** [ 0.937, 3.142]\n0.601    [ -0.674, 1.910]\n0.154    [ -1.377, 1.713]\n0.564    [ -1.043, 2.203]\n\n\ngenderMale\n0.016    [-0.433, 0.478]\n0.109    [ -0.358, 0.590]\n0.174    [ -0.303, 0.665]\n0.149    [ -0.331, 0.643]\n0.159    [ -0.339, 0.673]\n\n\nage_class35-44\n0.092    [-0.458, 0.641]\n-0.096    [ -0.689, 0.492]\n-0.051    [ -0.658, 0.553]\n-0.072    [ -0.681, 0.533]\n-0.025    [ -0.663, 0.610]\n\n\nage_class45-54\n0.781*   [ 0.121, 1.467]\n0.477    [ -0.234, 1.207]\n0.529    [ -0.195, 1.272]\n0.532    [ -0.192, 1.273]\n0.719    [ -0.045, 1.504]\n\n\nage_class55-64\n0.507    [-0.107, 1.129]\n0.241    [ -0.450, 0.937]\n0.332    [ -0.374, 1.042]\n0.325    [ -0.381, 1.035]\n0.421    [ -0.314, 1.162]\n\n\nage_class65-74\n1.409**  [ 0.463, 2.493]\n1.109*   [ 0.094, 2.247]\n1.134*   [ 0.103, 2.284]\n1.110*   [ 0.079, 2.260]\n1.176*   [ 0.121, 2.347]\n\n\nage_class75+\n-0.171    [-1.776, 1.539]\n-0.293    [ -1.939, 1.447]\n-0.209    [ -1.903, 1.594]\n-0.218    [ -1.920, 1.588]\n0.155    [ -1.662, 2.126]\n\n\neduMedi\n-0.339    [-0.780, 0.101]\n-0.349    [ -0.871, 0.168]\n-0.298    [ -0.833, 0.234]\n-0.305    [ -0.842, 0.229]\n-0.328    [ -0.883, 0.223]\n\n\nchronicNo\n0.046    [-0.442, 0.521]\n-0.009    [ -0.515, 0.483]\n0.099    [ -0.417, 0.604]\n0.090    [ -0.427, 0.596]\n0.134    [ -0.397, 0.653]\n\n\nmemberNo\n-\n-0.353    [ -0.991, 0.269]\n-0.291    [ -0.948, 0.348]\n-0.291    [ -0.949, 0.350]\n-0.272    [ -0.948, 0.388]\n\n\nprofNurs\n-\n15.539    [-33.395, NA]\n15.660    [-31.688, NA]\n15.693    [-31.626, NA]\n15.763    [-30.403, NA]\n\n\nprofNutr\n-\n-0.278    [ -0.849, 0.287]\n-0.229    [ -0.811, 0.349]\n-0.224    [ -0.807, 0.355]\n-0.151    [ -0.765, 0.460]\n\n\nprofPhar\n-\n-0.026    [ -0.684, 0.637]\n-0.093    [ -0.763, 0.583]\n-0.100    [ -0.772, 0.578]\n-0.180    [ -0.875, 0.518]\n\n\nprofPhys\n-\n0.009    [ -0.731, 0.799]\n-0.030    [ -0.788, 0.776]\n-0.037    [ -0.796, 0.769]\n-0.034    [ -0.817, 0.796]\n\n\nw_settingComm\n-\n-0.826*   [ -1.659, -0.035]\n-0.783    [ -1.631, 0.024]\n-0.794    [ -1.642, 0.014]\n-0.773    [ -1.649, 0.063]\n\n\nw_settingHosp\n-\n-1.114**  [ -1.952, -0.321]\n-1.039*   [ -1.896, -0.225]\n-1.036*   [ -1.894, -0.221]\n-0.983*   [ -1.873, -0.131]\n\n\nw_settingPriv\n-\n-0.816*   [ -1.629, -0.053]\n-0.793    [ -1.619, -0.015]\n-0.775    [ -1.604, 0.004]\n-0.563    [ -1.415, 0.248]\n\n\nw_settingUniv\n-\n-0.944*   [ -1.798, -0.140]\n-1.006*   [ -1.884, -0.179]\n-1.031*   [ -1.912, -0.201]\n-1.107*   [ -2.014, -0.253]\n\n\nfear\n-\n-\n0.152*** [ 0.081, 0.224]\n0.150*** [ 0.079, 0.222]\n0.094*   [ 0.010, 0.176]\n\n\nknowledge\n-\n-\n-\n0.067    [ -0.061, 0.195]\n0.054    [ -0.083, 0.190]\n\n\nML2\n-\n-\n-\n-\n-0.182    [ -0.757, 0.380]\n\n\nML3\n-\n-\n-\n-\n0.399    [ -0.005, 0.804]\n\n\nML5\n-\n-\n-\n-\n0.485    [ -0.046, 1.023]\n\n\nML4\n-\n-\n-\n-\n-0.016    [ -0.425, 0.392]\n\n\nML1\n-\n-\n-\n-\n0.281*   [ 0.034, 0.534]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 Sentiment 4 (S4)\n\ns4_models &lt;- map(s1_models, \n                 ~ update(.x, change_vac_3 ~ .))\n\ns4_result_comp &lt;- map(s4_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(s4_result_comp) &lt;- c(\"EVs\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\")\n\ngt(s4_result_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 8: Sentiment 4 result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n2.383*** [ 1.590, 3.251]\n1.889**  [ 0.697, 3.155]\n0.372    [-1.066, 1.852]\n0.969    [-0.865, 2.874]\n1.186    [-0.718, 3.163]\n\n\ngenderMale\n-0.333    [-0.862, 0.217]\n-0.260    [-0.807, 0.310]\n-0.212    [-0.768, 0.365]\n-0.183    [-0.742, 0.397]\n-0.159    [-0.734, 0.439]\n\n\nage_class35-44\n-0.271    [-1.000, 0.433]\n-0.269    [-1.040, 0.472]\n-0.211    [-0.998, 0.550]\n-0.193    [-0.983, 0.571]\n-0.104    [-0.925, 0.697]\n\n\nage_class45-54\n0.120    [-0.712, 0.966]\n0.175    [-0.717, 1.080]\n0.182    [-0.720, 1.095]\n0.180    [-0.726, 1.098]\n0.297    [-0.634, 1.241]\n\n\nage_class55-64\n0.049    [-0.751, 0.843]\n0.068    [-0.817, 0.948]\n0.153    [-0.745, 1.047]\n0.160    [-0.743, 1.059]\n0.203    [-0.723, 1.123]\n\n\nage_class65-74\n0.444    [-0.650, 1.677]\n0.416    [-0.765, 1.717]\n0.359    [-0.831, 1.664]\n0.392    [-0.806, 1.706]\n0.479    [-0.759, 1.836]\n\n\nage_class75+\n0.037    [-1.906, 3.045]\n0.253    [-1.765, 3.299]\n0.335    [-1.723, 3.404]\n0.370    [-1.706, 3.448]\n0.530    [-1.597, 3.644]\n\n\neduMedi\n-0.524    [-1.058, 0.008]\n-0.424    [-1.045, 0.198]\n-0.356    [-0.996, 0.286]\n-0.350    [-0.990, 0.291]\n-0.315    [-0.971, 0.340]\n\n\nchronicNo\n-0.442    [-1.104, 0.166]\n-0.377    [-1.051, 0.245]\n-0.277    [-0.963, 0.361]\n-0.280    [-0.968, 0.360]\n-0.246    [-0.944, 0.405]\n\n\nmemberNo\n-\n0.434    [-0.245, 1.112]\n0.548    [-0.155, 1.255]\n0.552    [-0.151, 1.258]\n0.631    [-0.091, 1.361]\n\n\nprofNurs\n-\n1.052    [-0.721, 4.006]\n1.258    [-0.606, 4.266]\n1.230    [-0.646, 4.244]\n1.111    [-0.695, 4.078]\n\n\nprofNutr\n-\n0.099    [-0.623, 0.829]\n0.175    [-0.557, 0.916]\n0.163    [-0.569, 0.904]\n0.291    [-0.484, 1.087]\n\n\nprofPhar\n-\n0.267    [-0.531, 1.096]\n0.208    [-0.600, 1.045]\n0.213    [-0.594, 1.051]\n0.103    [-0.730, 0.964]\n\n\nprofPhys\n-\n-0.230    [-0.995, 0.579]\n-0.279    [-1.059, 0.544]\n-0.265    [-1.047, 0.559]\n-0.336    [-1.133, 0.502]\n\n\nw_settingComm\n-\n-0.218    [-1.102, 0.651]\n-0.120    [-1.017, 0.765]\n-0.103    [-1.003, 0.785]\n0.006    [-0.920, 0.928]\n\n\nw_settingHosp\n-\n-0.127    [-1.065, 0.818]\n0.024    [-0.934, 0.994]\n0.033    [-0.928, 1.007]\n0.192    [-0.799, 1.204]\n\n\nw_settingPriv\n-\n0.325    [-0.620, 1.278]\n0.409    [-0.550, 1.377]\n0.400    [-0.561, 1.370]\n0.623    [-0.370, 1.636]\n\n\nw_settingUniv\n-\n-0.126    [-1.007, 0.721]\n-0.124    [-1.030, 0.749]\n-0.094    [-1.002, 0.781]\n-0.070    [-0.986, 0.817]\n\n\nfear\n-\n-\n0.159*** [ 0.076, 0.242]\n0.161*** [ 0.078, 0.243]\n0.103*   [ 0.006, 0.198]\n\n\nknowledge\n-\n-\n-\n-0.087    [-0.258, 0.077]\n-0.084    [-0.262, 0.086]\n\n\nML2\n-\n-\n-\n-\n-0.200    [-0.872, 0.451]\n\n\nML3\n-\n-\n-\n-\n0.095    [-0.375, 0.550]\n\n\nML5\n-\n-\n-\n-\n0.714*   [ 0.116, 1.312]\n\n\nML4\n-\n-\n-\n-\n-0.092    [-0.549, 0.364]\n\n\nML1\n-\n-\n-\n-\n0.117    [-0.170, 0.407]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Sentiment 5 (S5)\n\ns5_models &lt;- map(s1_models, \n                 ~ update(.x, cov_pro_3 ~ .))\n\ns5_result_comp &lt;- map(s5_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(s5_result_comp) &lt;- c(\"EVs\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\")\n\ngt(s5_result_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 9: Sentiment 5 result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n1.377*** [ 0.673, 2.130]\n3.290*** [ 1.919, 4.786]\n0.561    [-1.084, 2.273]\n-0.786    [-2.774, 1.237]\n0.501    [-2.185, 3.264]\n\n\ngenderMale\n-0.100    [-0.620, 0.445]\n-0.059    [-0.626, 0.533]\n0.127    [-0.495, 0.778]\n0.043    [-0.589, 0.701]\n0.099    [-0.766, 1.013]\n\n\nage_class35-44\n-0.159    [-0.787, 0.458]\n-0.412    [-1.102, 0.260]\n-0.396    [-1.155, 0.345]\n-0.429    [-1.189, 0.314]\n-0.028    [-1.081, 1.022]\n\n\nage_class45-54\n0.225    [-0.511, 0.983]\n-0.184    [-1.005, 0.652]\n-0.147    [-1.050, 0.765]\n-0.118    [-1.022, 0.796]\n0.653    [-0.539, 1.883]\n\n\nage_class55-64\n0.643    [-0.120, 1.441]\n-0.041    [-0.903, 0.838]\n0.121    [-0.818, 1.074]\n0.160    [-0.790, 1.129]\n1.037    [-0.228, 2.347]\n\n\nage_class65-74\n1.586*   [ 0.251, 3.471]\n0.649    [-0.787, 2.582]\n0.961    [-0.647, 3.011]\n0.999    [-0.621, 3.053]\n0.914    [-0.848, 3.075]\n\n\nage_class75+\n-0.600    [-2.275, 1.429]\n-1.840    [-3.765, 0.341]\n-1.713    [-3.871, 0.690]\n-1.738    [-3.999, 0.746]\n-0.736    [-3.525, 2.242]\n\n\neduMedi\n0.283    [-0.246, 0.829]\n-0.408    [-1.041, 0.230]\n-0.346    [-1.045, 0.361]\n-0.383    [-1.091, 0.332]\n-0.132    [-1.128, 0.868]\n\n\nchronicNo\n-0.106    [-0.726, 0.473]\n-0.346    [-1.022, 0.282]\n-0.117    [-0.842, 0.567]\n-0.147    [-0.879, 0.542]\n-0.380    [-1.389, 0.553]\n\n\nmemberNo\n-\n-0.763    [-1.749, 0.128]\n-0.452    [-1.515, 0.526]\n-0.437    [-1.504, 0.547]\n0.273    [-0.986, 1.494]\n\n\nprofNurs\n-\n-1.916*   [-3.382, -0.393]\n-1.941*   [-3.479, -0.339]\n-1.840*   [-3.394, -0.228]\n-1.667    [-3.716, 0.413]\n\n\nprofNutr\n-\n-0.414    [-1.072, 0.229]\n-0.390    [-1.112, 0.316]\n-0.350    [-1.078, 0.362]\n-0.139    [-1.120, 0.830]\n\n\nprofPhar\n-\n0.804    [-0.128, 1.795]\n0.667    [-0.329, 1.722]\n0.726    [-0.287, 1.800]\n-0.265    [-1.670, 1.198]\n\n\nprofPhys\n-\n0.225    [-0.775, 1.392]\n0.150    [-0.959, 1.425]\n0.096    [-1.007, 1.363]\n-0.426    [-1.835, 1.164]\n\n\nw_settingComm\n-\n-0.809    [-1.738, 0.074]\n-0.771    [-1.774, 0.187]\n-0.816    [-1.829, 0.148]\n-0.455    [-1.737, 0.781]\n\n\nw_settingHosp\n-\n-0.844    [-1.779, 0.044]\n-0.735    [-1.756, 0.246]\n-0.757    [-1.793, 0.238]\n-0.084    [-1.480, 1.305]\n\n\nw_settingPriv\n-\n-0.677    [-1.582, 0.169]\n-0.710    [-1.684, 0.208]\n-0.695    [-1.682, 0.235]\n0.309    [-0.944, 1.533]\n\n\nw_settingUniv\n-\n0.523    [-0.569, 1.647]\n0.565    [-0.645, 1.805]\n0.529    [-0.699, 1.783]\n0.400    [-1.047, 1.885]\n\n\nfear\n-\n-\n0.299*** [ 0.217, 0.385]\n0.300*** [ 0.217, 0.387]\n0.148*   [ 0.022, 0.274]\n\n\nknowledge\n-\n-\n-\n0.195*   [ 0.037, 0.356]\n0.095    [-0.123, 0.317]\n\n\nML2\n-\n-\n-\n-\n0.819    [-0.009, 1.667]\n\n\nML3\n-\n-\n-\n-\n0.378    [-0.207, 0.970]\n\n\nML5\n-\n-\n-\n-\n1.467*** [ 0.719, 2.266]\n\n\nML4\n-\n-\n-\n-\n-0.284    [-0.877, 0.293]\n\n\nML1\n-\n-\n-\n-\n-0.328    [-0.813, 0.141]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.6 Summary: Model 5 in each sentiment\n\nm5_comp &lt;- map(list(s1_models[[5]], s2_models[[5]], s3_models[[5]], s4_models[[5]], s5_models[[5]]), \n               extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(m5_comp) &lt;- c(\"EVs\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\")\n\ngt(m5_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything()) \n\n\n\nTable 10: Model 5 summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nS1\nS2\nS3\nS4\nS5\n\n\n\n\n(Intercept)\n-0.793    [-2.394, 0.815]\n0.388    [-1.398, 2.212]\n0.564    [ -1.043, 2.203]\n1.186    [-0.718, 3.163]\n0.501    [-2.185, 3.264]\n\n\ngenderMale\n0.123    [-0.379, 0.641]\n0.081    [-0.492, 0.676]\n0.159    [ -0.339, 0.673]\n-0.159    [-0.734, 0.439]\n0.099    [-0.766, 1.013]\n\n\nage_class35-44\n0.698*   [ 0.091, 1.314]\n0.050    [-0.692, 0.787]\n-0.025    [ -0.663, 0.610]\n-0.104    [-0.925, 0.697]\n-0.028    [-1.081, 1.022]\n\n\nage_class45-54\n0.727*   [ 0.030, 1.443]\n0.224    [-0.611, 1.067]\n0.719    [ -0.045, 1.504]\n0.297    [-0.634, 1.241]\n0.653    [-0.539, 1.883]\n\n\nage_class55-64\n1.190**  [ 0.452, 1.954]\n-0.117    [-0.950, 0.708]\n0.421    [ -0.314, 1.162]\n0.203    [-0.723, 1.123]\n1.037    [-0.228, 2.347]\n\n\nage_class65-74\n2.361*** [ 1.139, 3.904]\n2.435*   [ 0.656, 5.408]\n1.176*   [ 0.121, 2.347]\n0.479    [-0.759, 1.836]\n0.914    [-0.848, 3.075]\n\n\nage_class75+\n-0.607    [-2.412, 1.111]\n0.392    [-1.718, 2.885]\n0.155    [ -1.662, 2.126]\n0.530    [-1.597, 3.644]\n-0.736    [-3.525, 2.242]\n\n\neduMedi\n-0.175    [-0.723, 0.372]\n-0.384    [-1.015, 0.243]\n-0.328    [ -0.883, 0.223]\n-0.315    [-0.971, 0.340]\n-0.132    [-1.128, 0.868]\n\n\nchronicNo\n0.579*   [ 0.059, 1.094]\n-0.077    [-0.712, 0.529]\n0.134    [ -0.397, 0.653]\n-0.246    [-0.944, 0.405]\n-0.380    [-1.389, 0.553]\n\n\nmemberNo\n0.153    [-0.505, 0.807]\n0.297    [-0.462, 1.053]\n-0.272    [ -0.948, 0.388]\n0.631    [-0.091, 1.361]\n0.273    [-0.986, 1.494]\n\n\nprofNurs\n-0.115    [-1.397, 1.226]\n0.762    [-0.889, 2.899]\n15.763    [-30.403, NA]\n1.111    [-0.695, 4.078]\n-1.667    [-3.716, 0.413]\n\n\nprofNutr\n0.256    [-0.338, 0.854]\n-0.169    [-0.875, 0.531]\n-0.151    [ -0.765, 0.460]\n0.291    [-0.484, 1.087]\n-0.139    [-1.120, 0.830]\n\n\nprofPhar\n0.393    [-0.348, 1.157]\n0.318    [-0.515, 1.178]\n-0.180    [ -0.875, 0.518]\n0.103    [-0.730, 0.964]\n-0.265    [-1.670, 1.198]\n\n\nprofPhys\n0.558    [-0.266, 1.483]\n0.007    [-0.854, 0.934]\n-0.034    [ -0.817, 0.796]\n-0.336    [-1.133, 0.502]\n-0.426    [-1.835, 1.164]\n\n\nw_settingComm\n0.358    [-0.420, 1.142]\n-0.494    [-1.409, 0.398]\n-0.773    [ -1.649, 0.063]\n0.006    [-0.920, 0.928]\n-0.455    [-1.737, 0.781]\n\n\nw_settingHosp\n0.402    [-0.408, 1.225]\n-0.346    [-1.304, 0.603]\n-0.983*   [ -1.873, -0.131]\n0.192    [-0.799, 1.204]\n-0.084    [-1.480, 1.305]\n\n\nw_settingPriv\n-0.152    [-0.898, 0.580]\n-0.059    [-0.960, 0.826]\n-0.563    [ -1.415, 0.248]\n0.623    [-0.370, 1.636]\n0.309    [-0.944, 1.533]\n\n\nw_settingUniv\n0.325    [-0.455, 1.104]\n-0.473    [-1.418, 0.440]\n-1.107*   [ -2.014, -0.253]\n-0.070    [-0.986, 0.817]\n0.400    [-1.047, 1.885]\n\n\nfear\n-0.043    [-0.133, 0.043]\n0.095*   [ 0.001, 0.187]\n0.094*   [ 0.010, 0.176]\n0.103*   [ 0.006, 0.198]\n0.148*   [ 0.022, 0.274]\n\n\nknowledge\n0.086    [-0.050, 0.223]\n0.046    [-0.110, 0.200]\n0.054    [ -0.083, 0.190]\n-0.084    [-0.262, 0.086]\n0.095    [-0.123, 0.317]\n\n\nML2\n0.107    [-0.462, 0.671]\n0.081    [-0.550, 0.706]\n-0.182    [ -0.757, 0.380]\n-0.200    [-0.872, 0.451]\n0.819    [-0.009, 1.667]\n\n\nML3\n-0.074    [-0.480, 0.321]\n0.571*   [ 0.125, 1.025]\n0.399    [ -0.005, 0.804]\n0.095    [-0.375, 0.550]\n0.378    [-0.207, 0.970]\n\n\nML5\n0.387    [-0.149, 0.922]\n0.719*   [ 0.140, 1.306]\n0.485    [ -0.046, 1.023]\n0.714*   [ 0.116, 1.312]\n1.467*** [ 0.719, 2.266]\n\n\nML4\n-0.131    [-0.549, 0.281]\n-0.125    [-0.587, 0.330]\n-0.016    [ -0.425, 0.392]\n-0.092    [-0.549, 0.364]\n-0.284    [-0.877, 0.293]\n\n\nML1\n0.020    [-0.219, 0.260]\n0.289    [ 0.000, 0.588]\n0.281*   [ 0.034, 0.534]\n0.117    [-0.170, 0.407]\n-0.328    [-0.813, 0.141]\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.7 Appendix: Model 5 with using the attitude variabes instead of 5 factors\n\nm5_ori_models &lt;- map(list(s1_models[[5]], s2_models[[5]], s3_models[[5]], s4_models[[5]], s5_models[[5]]),\n                      ~ update(.x, as.formula(str_c(\". ~ .\", \n                                              \" - \", str_flatten(fa_var, \" - \"), \n                                              \" + \", str_flatten(attitude_var, \" + \")))))\n\nm5_ori_comp &lt;- map(m5_ori_models, extract_esti) |&gt; \n  reduce(full_join, by = \"vars\")\nnames(m5_ori_comp) &lt;- c(\"EVs\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\")\n\ngt(m5_ori_comp) |&gt; \n  sub_missing(missing_text = \"-\") |&gt; \n  fmt_markdown(columns = everything())\n\n\n\nTable 11: Model 5 with using the attitude variabes instead of 5 factors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEVs\nS1\nS2\nS3\nS4\nS5\n\n\n\n\n(Intercept)\n-2.674    [-6.336, 0.898]\n-7.843*** [-12.447, -3.545]\n-5.286**  [ -9.337, -1.405]\n-1.472    [-5.904, 2.869]\n-6.815*   [-13.300, -0.550]\n\n\ngenderMale\n0.162    [-0.354, 0.695]\n0.130    [ -0.467, 0.751]\n0.218    [ -0.308, 0.762]\n-0.108    [-0.702, 0.508]\n0.103    [ -0.846, 1.102]\n\n\nage_class35-44\n0.630*   [ 0.008, 1.258]\n0.003    [ -0.772, 0.773]\n-0.034    [ -0.706, 0.634]\n-0.101    [-0.954, 0.734]\n-0.016    [ -1.158, 1.121]\n\n\nage_class45-54\n0.654    [-0.065, 1.391]\n0.131    [ -0.743, 1.013]\n0.621    [ -0.174, 1.435]\n0.438    [-0.538, 1.433]\n0.898    [ -0.450, 2.321]\n\n\nage_class55-64\n1.070**  [ 0.306, 1.859]\n-0.220    [ -1.099, 0.650]\n0.419    [ -0.353, 1.196]\n0.274    [-0.698, 1.244]\n0.999    [ -0.397, 2.449]\n\n\nage_class65-74\n2.368*** [ 1.106, 3.941]\n2.789*   [ 0.792, 5.996]\n1.348*   [ 0.228, 2.579]\n0.797    [-0.517, 2.229]\n0.830    [ -1.222, 3.237]\n\n\nage_class75+\n-1.086    [-2.975, 0.704]\n0.289    [ -1.909, 2.864]\n0.104    [ -1.822, 2.201]\n0.285    [-1.900, 3.422]\n-1.023    [ -3.753, 1.944]\n\n\neduMedi\n-0.127    [-0.687, 0.433]\n-0.310    [ -0.968, 0.343]\n-0.248    [ -0.823, 0.325]\n-0.354    [-1.031, 0.318]\n-0.397    [ -1.499, 0.697]\n\n\nchronicNo\n0.540*   [ 0.007, 1.068]\n-0.034    [ -0.695, 0.598]\n0.213    [ -0.343, 0.760]\n-0.176    [-0.890, 0.494]\n-0.533    [ -1.620, 0.472]\n\n\nmemberNo\n0.035    [-0.638, 0.704]\n0.276    [ -0.514, 1.066]\n-0.182    [ -0.888, 0.511]\n0.534    [-0.205, 1.278]\n0.465    [ -0.932, 1.828]\n\n\nprofNurs\n-0.161    [-1.491, 1.224]\n0.854    [ -0.977, 3.156]\n15.782    [-28.241, NA]\n1.059    [-0.803, 4.055]\n-2.835*   [ -5.272, -0.401]\n\n\nprofNutr\n0.303    [-0.312, 0.923]\n-0.196    [ -0.942, 0.544]\n-0.267    [ -0.919, 0.381]\n0.237    [-0.571, 1.062]\n-0.068    [ -1.164, 1.016]\n\n\nprofPhar\n0.399    [-0.355, 1.177]\n0.333    [ -0.522, 1.213]\n-0.273    [ -1.008, 0.465]\n0.089    [-0.781, 0.988]\n-0.405    [ -1.880, 1.105]\n\n\nprofPhys\n0.500    [-0.345, 1.444]\n-0.146    [ -1.047, 0.817]\n-0.152    [ -0.977, 0.717]\n-0.311    [-1.145, 0.563]\n-0.707    [ -2.263, 1.022]\n\n\nw_settingComm\n0.361    [-0.432, 1.161]\n-0.401    [ -1.342, 0.524]\n-0.611    [ -1.508, 0.250]\n0.157    [-0.795, 1.107]\n-0.733    [ -2.147, 0.631]\n\n\nw_settingHosp\n0.458    [-0.378, 1.309]\n-0.327    [ -1.311, 0.651]\n-0.887    [ -1.805, -0.003]\n0.314    [-0.722, 1.381]\n0.001    [ -1.502, 1.529]\n\n\nw_settingPriv\n-0.239    [-1.011, 0.518]\n-0.017    [ -0.941, 0.897]\n-0.421    [ -1.299, 0.418]\n0.898    [-0.134, 1.961]\n0.605    [ -0.779, 2.004]\n\n\nw_settingUniv\n0.281    [-0.518, 1.077]\n-0.403    [ -1.374, 0.543]\n-0.946*   [ -1.877, -0.066]\n-0.118    [-1.056, 0.789]\n0.604    [ -0.984, 2.249]\n\n\nfear\n-0.038    [-0.131, 0.050]\n0.095    [ -0.003, 0.191]\n0.092*   [ 0.004, 0.178]\n0.107*   [ 0.007, 0.206]\n0.138    [ -0.001, 0.278]\n\n\nknowledge\n0.092    [-0.048, 0.234]\n0.010    [ -0.155, 0.173]\n0.044    [ -0.101, 0.189]\n-0.100    [-0.289, 0.080]\n0.127    [ -0.111, 0.373]\n\n\nvac_gen_1\n0.068    [-0.566, 0.678]\n1.103**  [ 0.431, 1.771]\n0.461    [ -0.146, 1.057]\n-0.058    [-0.829, 0.656]\n0.325    [ -0.673, 1.252]\n\n\nvac_gen_2\n0.052    [-0.700, 0.788]\n-0.287    [ -1.118, 0.495]\n-0.175    [ -0.960, 0.569]\n0.020    [-0.923, 0.897]\n-0.181    [ -1.358, 0.954]\n\n\nvac_gen_3\n0.010    [-0.400, 0.424]\n0.115    [ -0.348, 0.583]\n0.173    [ -0.237, 0.584]\n0.156    [-0.338, 0.659]\n-0.576    [ -1.345, 0.159]\n\n\nvac_gen_4\n-0.030    [-0.446, 0.381]\n0.221    [ -0.254, 0.704]\n0.257    [ -0.169, 0.693]\n-0.053    [-0.550, 0.440]\n0.340    [ -0.409, 1.117]\n\n\nvac_gen_5\n-0.095    [-0.429, 0.227]\n0.196    [ -0.189, 0.569]\n0.709*** [ 0.398, 1.028]\n0.281    [-0.090, 0.642]\n0.563    [ -0.087, 1.217]\n\n\nvac_gen_6\n-0.082    [-0.362, 0.180]\n-0.044    [ -0.357, 0.247]\n-0.045    [ -0.313, 0.211]\n0.066    [-0.257, 0.366]\n0.020    [ -0.489, 0.490]\n\n\ncovid_pro_1\n0.043    [-0.475, 0.527]\n0.067    [ -0.521, 0.608]\n0.054    [ -0.489, 0.565]\n-0.705    [-1.455, -0.038]\n0.136    [ -0.807, 0.991]\n\n\ncovid_ag_1\n-0.040    [-0.320, 0.256]\n0.498*   [ 0.094, 0.952]\n-0.082    [ -0.381, 0.235]\n0.179    [-0.203, 0.611]\n-0.501*   [ -0.962, -0.030]\n\n\ncovid_ag_2\n0.266    [-0.154, 0.713]\n-0.002    [ -0.455, 0.483]\n0.203    [ -0.217, 0.646]\n0.026    [-0.444, 0.525]\n-0.991**  [ -1.661, -0.315]\n\n\ncovid_pro_2\n0.302    [-0.325, 0.944]\n-0.083    [ -0.869, 0.644]\n0.000    [ -0.725, 0.672]\n0.458    [-0.299, 1.171]\n-0.049    [ -1.174, 1.008]\n\n\ncovid_over18\n-0.013    [-0.476, 0.470]\n-0.225    [ -0.762, 0.344]\n0.032    [ -0.447, 0.541]\n-0.127    [-0.678, 0.458]\n0.333    [ -0.517, 1.241]\n\n\ncovid_pregnant\n0.209    [-0.201, 0.604]\n0.302    [ -0.177, 0.756]\n-0.192    [ -0.685, 0.252]\n-0.070    [-0.668, 0.464]\n0.607    [ -0.120, 1.294]\n\n\ncovid_children\n0.080    [-0.537, 0.701]\n0.006    [ -0.724, 0.749]\n0.088    [ -0.585, 0.768]\n0.388    [-0.389, 1.181]\n0.656    [ -0.362, 1.726]\n\n\ncovid_oblig_1\n0.433*   [ 0.025, 0.839]\n0.512*   [ 0.066, 0.956]\n0.143    [ -0.290, 0.562]\n0.669**  [ 0.215, 1.133]\n1.230*** [ 0.570, 1.930]\n\n\ncovid_oblig_2\n-0.148    [-0.458, 0.146]\n-0.064    [ -0.413, 0.266]\n0.125    [ -0.170, 0.412]\n0.042    [-0.321, 0.386]\n-0.376    [ -0.936, 0.145]\n\n\ncovid_leaders\n0.109    [-0.457, 0.668]\n0.502    [ -0.125, 1.142]\n0.486    [ -0.091, 1.078]\n-0.412    [-1.109, 0.251]\n0.677    [ -0.201, 1.590]\n\n\ncovid_earlyaccess\n-0.507    [-1.211, 0.167]\n-0.111    [ -0.868, 0.623]\n-0.274    [ -0.991, 0.419]\n0.031    [-0.752, 0.799]\n-0.667    [ -1.772, 0.365]"
  },
  {
    "objectID": "posts/20230703_welcome/index.html",
    "href": "posts/20230703_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog, where I’ll be sharing my academic research progress, including my papers, study notes, and my work with R language, statistics, and psychology. Of course, I’ll also be sharing my personal thoughts and experiences.\nSo, why I want to build a personal blog. The reason is simple. It’s time to share what I know!\n\n\n\nWhen you’ve written the same code 3 times, write a functionWhen you’ve given the same in-person advice 3 times, write a blog post\n\n— David Robinson (@drob) November 9, 2017\n\n\n\nAlthough I haven’t quite figured out what this blog will look like yet, let’s get started and worry about the details later. Anyway, thanks for joining me on this journey!\n\n\n\nThe sunrise in Kinderdijk (Children dike), the Netherlands"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBrazilian Vaccine Attitude Analysis\n\n\nAn example of doing factor analysis and logistic regession\n\n\n\nr\n\n\nstatistics\n\n\nconsultant\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nTzu-Yao Lin\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logistic Regression Aanalysis\n\n\n\n\n\n\nr\n\n\nstatistics\n\n\nconsultant\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nTzu-Yao Lin\n\n\n\n\n\n\n\n\n\n\n\n\nResource to find a PhD (or researcher) position\n\n\n\n\n\n\nacademia\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nTzu-Yao Lin\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nTzu-Yao Lin\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a website to record your research\n\n\n\n\n\n\nCCT\n\n\nrmarkdown\n\n\nblogdown\n\n\nnetlify\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nTzu-Yao Lin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Tzu-Yao Lin",
    "section": "",
    "text": "Hi there! I’m a joint PhD student at Maastricht University and KU Leuven, studying reliability for intensive longitudinal data. I’m also an R enthusiast, a fire dancer, and a Lindy hopper. In my blog, I aim to share useful knowledge and interesting things. On one hand, I hope to explain concepts in simple language. On the other hand, I want to organize my thoughts structurally and deepen their place in my memory. I firmly believe that writing aids thinking, and now is the time to put that into practice – even though I’ve disliked writing for as long as I can remember!"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "Tzu-Yao Lin",
    "section": "",
    "text": "Hi there! I’m a joint PhD student at Maastricht University and KU Leuven, studying reliability for intensive longitudinal data. I’m also an R enthusiast, a fire dancer, and a Lindy hopper. In my blog, I aim to share useful knowledge and interesting things. On one hand, I hope to explain concepts in simple language. On the other hand, I want to organize my thoughts structurally and deepen their place in my memory. I firmly believe that writing aids thinking, and now is the time to put that into practice – even though I’ve disliked writing for as long as I can remember!"
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Tzu-Yao Lin",
    "section": "Research interests",
    "text": "Research interests\ndecision-making reliability mathematical or computational modeling intensive longitudinal data analysis"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Tzu-Yao Lin",
    "section": "Education",
    "text": "Education\nKU Leuven | Belgium\nPhD candidate at Research Group of Quantitative Psychology and Individual Differences | Oct. 2022 - Present\nMaastricht University | The Netherlands\nPhD candidate at Department of Methodology and Statistics | Oct. 2022 - Present\nNational Taiwan University | Taiwan\nMS in Psychology | Sep. 2019 - Jun. 2022\nNational Taiwan University | Taiwan\nBS in Psychology (major) and Forestry (minor) | Sep. 2014 - Jun. 2019"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Tzu-Yao Lin",
    "section": "Skills",
    "text": "Skills\nProgramming: R, Linux, Stan, Python, Matlab Language: Mandarin, English, Dutch (new learner)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there! Welcome.",
    "section": "",
    "text": "About this blog\nThis is the contents of the about page for my blog."
  },
  {
    "objectID": "posts/20240918_CDP/index.html",
    "href": "posts/20240918_CDP/index.html",
    "title": "Multinomial Logistic Regression Aanalysis",
    "section": "",
    "text": "Load packages and data (Final_data.csv) which were preprocessed by YuFong.\n\n\nCode\nlibrary(tidyverse)\nlibrary(nnet)\nlibrary(epiDisplay)\nlibrary(lmtest)\nlibrary(gt)\n\n\n\n\nCode\ndata &lt;- read_csv(\"Final_data.csv\", na = c(\"\", \"NA\", \"-999\", \"-888\"))\n\n\nHere, I retain the variables that will be analyzed in following.\n\n\nCode\ndiabetes_with_na &lt;- data |&gt;\n    mutate(Diabetes = factor(N_Diabetes_WHO, labels = c(\"NGM\", \"Pre\", \"T2DM\"), ordered = TRUE),\n           IsDiabetes = factor(N_DIABETES_2b),\n           PRS = factor(PRS_tertiles, labels = c(\"Low\", \"Medium\", \"High\")), \n           Coffee = factor(coffee_tertiles, labels = c(\"Low\", \"Medium\", \"High\")),\n           Sex = factor(SEX, labels = c(\"Male\", \"Female\")),\n           Age = Age,\n           BMI = bmi,\n           Alcohol = NIT_alcoholtot,\n           Smoking = factor(smoking_3cat, labels = c(\"Never\", \"Former\", \"Current\")),\n           Step_wake = mean_step_min_wake_T,\n           DHD = DHD_sum, \n           KCal = NIT_kcal, \n           CVD = factor(N_CVD, labels = c(\"No\", \"Yes\")),\n           Chol = Tot_chol,\n           # HT = factor(N_HT, labels = c(\"No\", \"Yes\")),\n           OMAP = N_OMAP,\n           Med_HT = factor(med_HT, labels = c(\"No\", \"Yes\")),\n           Med_LP = factor(med_LP, labels = c(\"No\", \"Yes\")),\n           Sugar = FIT_suikerg,\n           B1_VD1 = factor(B1_VD1_2.6.29, labels = c(\"Yes\", \"No\", \"Unknow\")),\n           Education = factor(N_Education_3cat, labels = c(\"Low\", \"Medium\", \"High\")),\n           .keep = \"none\") \n\nskimr::skim(diabetes_with_na)\n\n\n\nData summary\n\n\nName\ndiabetes_with_na\n\n\nNumber of rows\n7668\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n11\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDiabetes\n0\n1.00\nTRUE\n3\nNGM: 4868, T2D: 1641, Pre: 1159\n\n\nIsDiabetes\n0\n1.00\nFALSE\n2\n0: 6027, 1: 1641\n\n\nPRS\n0\n1.00\nFALSE\n3\nMed: 2561, Low: 2556, Hig: 2551\n\n\nCoffee\n0\n1.00\nFALSE\n3\nLow: 2949, Med: 2369, Hig: 2350\n\n\nSex\n0\n1.00\nFALSE\n2\nFem: 3858, Mal: 3810\n\n\nSmoking\n56\n0.99\nFALSE\n3\nFor: 3764, Nev: 2911, Cur: 937\n\n\nCVD\n86\n0.99\nFALSE\n2\nNo: 6342, Yes: 1240\n\n\nMed_HT\n7\n1.00\nFALSE\n2\nNo: 4904, Yes: 2757\n\n\nMed_LP\n7\n1.00\nFALSE\n2\nNo: 5390, Yes: 2271\n\n\nB1_VD1\n97\n0.99\nFALSE\n3\nNo: 4082, Yes: 2417, Unk: 1072\n\n\nEducation\n100\n0.99\nFALSE\n3\nHig: 2984, Low: 2511, Med: 2073\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1.00\n59.93\n8.64\n39.00\n54.00\n61.00\n67.00\n79.00\n▃▅▇▇▂\n\n\nBMI\n2\n1.00\n26.80\n4.41\n14.40\n23.80\n26.20\n29.10\n56.30\n▂▇▂▁▁\n\n\nAlcohol\n0\n1.00\n11.82\n13.35\n0.00\n1.60\n8.10\n18.01\n197.53\n▇▁▁▁▁\n\n\nStep_wake\n1295\n0.83\n118.91\n40.26\n5.14\n90.72\n116.32\n144.69\n314.36\n▁▇▅▁▁\n\n\nDHD\n0\n1.00\n84.02\n15.26\n10.76\n73.60\n84.30\n94.91\n131.17\n▁▁▇▇▁\n\n\nKCal\n0\n1.00\n2121.23\n603.92\n594.18\n1691.53\n2048.51\n2491.85\n4178.56\n▁▇▇▂▁\n\n\nChol\n1\n1.00\n5.22\n1.10\n2.10\n4.40\n5.20\n6.00\n10.40\n▂▇▆▁▁\n\n\nOMAP\n5\n1.00\n94.70\n11.38\n61.00\n87.00\n94.00\n102.00\n145.00\n▁▇▇▁▁\n\n\nSugar\n0\n1.00\n2.62\n7.22\n0.00\n0.00\n0.00\n2.15\n120.00\n▇▁▁▁▁\n\n\n\n\n\nYou may notice that there are some misssing. However, the Step_wake has more than a thousand of missing. It will be influenced if I remove all of these missing data.\n\n\nCode\ndiabetes &lt;- diabetes_with_na |&gt; drop_na()\n\nprint(diabetes, width = Inf)\n\n\n# A tibble: 6,224 × 20\n     Age Diabetes IsDiabetes PRS    Coffee Sex      BMI Alcohol Smoking\n   &lt;dbl&gt; &lt;ord&gt;    &lt;fct&gt;      &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;  \n 1    47 NGM      0          Low    Medium Female  22.1  14.0   Current\n 2    68 NGM      0          Low    Medium Female  23     7.78  Never  \n 3    58 NGM      0          Medium Low    Female  30.8  16.6   Former \n 4    59 NGM      0          Low    High   Male    35.6   1.91  Former \n 5    62 Pre      0          High   Low    Male    26.4  19.0   Former \n 6    68 NGM      0          High   Medium Male    21.5  16.3   Former \n 7    58 NGM      0          Medium Medium Male    25.6  18.3   Former \n 8    43 NGM      0          Low    Medium Female  20.9   0.606 Former \n 9    68 NGM      0          Medium Low    Female  25.6  13.3   Former \n10    47 NGM      0          Low    High   Male    29.4  19.9   Current\n   Step_wake   DHD  KCal CVD    Chol  OMAP Med_HT Med_LP Sugar B1_VD1 Education\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    \n 1     165.   93.8 2635. No      6.6    89 No     No      0    No     Medium   \n 2     152.  101.  3149. No      5.3    92 Yes    No      0    No     High     \n 3     137.   98.8 1934. No      6.3   100 Yes    No      0    Unknow High     \n 4      58.5  90.3 2058. No      3.9   120 Yes    No      1.4  No     High     \n 5      51.5  67.6 2525. No      5     103 No     No      0    Unknow Medium   \n 6      77.0  69.6 1924. No      5.2   103 Yes    Yes    20    No     High     \n 7     140.   80.7 2622. Yes     4.8   100 Yes    Yes     1.45 No     High     \n 8     163.   68.0 2310. No      4.1    81 No     No     20    No     Medium   \n 9      77.4  94.4 1402. Yes     8.1    95 No     No      0    No     Low      \n10     106.   82.0 2278. No      4.4    85 No     No      1.45 No     Medium   \n# ℹ 6,214 more rows\n\n\n\n\n\n\n\n\nMissing data\n\n\n\nI’m not sure how Yufong dealt with the missing data. Here, I remove the subject’s data if he/she has at least one missing value (denoted by NA, -999, or -888) in the selected 20 variables.\nThere is another issue if we want to make model selection or comparison by AICs or likelihood ratio tests, we need to have the same data (after removing the missing) when fitting to each model.\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the reason that we make PRS and Coffee variable from a continuous scale to three ordinal categories (low, medium, and high) for each? Why not four or more, or just keep it continuous? I think it needs more explanation (Testing for trend).\nDoes it have any advantage of using tertiles to separate three categories?"
  },
  {
    "objectID": "posts/20240918_CDP/index.html#models",
    "href": "posts/20240918_CDP/index.html#models",
    "title": "Multinomial Logistic Regression Aanalysis",
    "section": "2.1 Models",
    "text": "2.1 Models\nThe dependent variable has three (ordinal?) categories. It is usually to use oridinal logistic regression to fit the data. One of the assumptions underlying ordinal logistic (and ordinal probit) regression is that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients that describe the relationship between, say, the lowest versus all higher categories of the response variable are the same as those that describe the relationship between the next lowest category and all higher categories, etc.\n\n\nCode\nolm &lt;- MASS::polr(Diabetes ~ PRS + Coffee + Age + Sex + BMI + Alcohol + Smoking + Step_wake + DHD + KCal + CVD + Chol + OMAP + Med_HT + Med_LP + Sugar + B1_VD1 + Education, data = diabetes, Hess = TRUE)\nbrant::brant(olm)\n\n\n---------------------------------------------------- \nTest for        X2  df  probability \n---------------------------------------------------- \nOmnibus         208.92  23  0\nPRSMedium       4.41    1   0.04\nPRSHigh         6.4 1   0.01\nCoffeeMedium        0.86    1   0.36\nCoffeeHigh      0.25    1   0.62\nAge         6.87    1   0.01\nSexFemale       7.62    1   0.01\nBMI         0.21    1   0.65\nAlcohol         5.78    1   0.02\nSmokingFormer       0.58    1   0.45\nSmokingCurrent      1.04    1   0.31\nStep_wake       5.9 1   0.02\nDHD         1.88    1   0.17\nKCal            0.95    1   0.33\nCVDYes          1.21    1   0.27\nChol            57.18   1   0\nOMAP            0.24    1   0.62\nMed_HTYes       0.4 1   0.53\nMed_LPYes       4.93    1   0.03\nSugar           9.85    1   0\nB1_VD1No        29.36   1   0\nB1_VD1Unknow        4.06    1   0.04\nEducationMedium 0.96    1   0.33\nEducationHigh       0.52    1   0.47\n---------------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nHowever, we found the assumption does not hold in our data. because it did not pass the parallel ratio check (link)?\nTherefore, we tried to use\n\\[\n\\begin{align}\n\\log\\left(\\frac{p_{\\text{Pre}}}{p_{\\text{NGM}}}\\right) = \\alpha_{1} + \\beta_{11} E_1 + \\gamma_{11} C \\\\\n\\log\\left(\\frac{p_{\\text{T2DM}}}{p_{\\text{NGM}}}\\right) = \\alpha_{2} + \\beta_{21} E_1 + \\gamma_{21} C\n\\end{align}\n\\]\nTwo binary vs. One multinomial\nThere are many potential models that are discussed in the manuscript. I list them in Table below.\n\n\n\nTable 1: Candidate models\n\n\n\n\n\n\n\n\n\n\n\nExposure\nCovariate set 1\nCovariate set 2\nCovariate set 3\n\n\n\n\nPRS\nprs1\nprs2\nprs3\n\n\nCof\ncof1\ncof2\ncof3\n\n\nPRS+Cof\nprs_cof1\nprs_cof2\nprs_cof3\n\n\nPRS+Cof+PRS*Cof\nprs_cof_i1\nprs_cof_i2\nprs_cof_i3\n\n\n\n\n\n\n\n\nCode\n## Check the PRS effect (Table 3)\nprs1 &lt;- formula(Diabetes ~ PRS + Age + Sex)\nprs2 &lt;- update(prs1, . ~ . + BMI + Alcohol + Smoking + Step_wake + DHD + KCal)\nprs3 &lt;- update(prs2, . ~ . + CVD + Chol + OMAP + Med_HT + Med_LP + Sugar + B1_VD1 + Education)\n\n## Check the Coffee effect (Table 4)\ncof1 &lt;- update(prs1, . ~ Coffee - PRS + .)\ncof2 &lt;- update(prs2, . ~ Coffee - PRS + .) \ncof3 &lt;- update(prs3, . ~ Coffee - PRS + .)\n\n## Check the PRS and Coffe effect (Table 5)\nprs_cof1 &lt;- update(prs1, . ~ Coffee + .)\nprs_cof2 &lt;- update(prs2, . ~ Coffee + .)    \nprs_cof3 &lt;- update(prs3, . ~ Coffee + .)\n\nprs_cof_i1 &lt;- update(prs_cof1, . ~ PRS:Coffee + .)\nprs_cof_i2 &lt;- update(prs_cof2, . ~ PRS:Coffee + .)\nprs_cof_i3 &lt;- update(prs_cof3, . ~ PRS:Coffee + .)"
  },
  {
    "objectID": "posts/20240918_CDP/index.html#check-the-prs-effect-reproduce-table-2",
    "href": "posts/20240918_CDP/index.html#check-the-prs-effect-reproduce-table-2",
    "title": "Multinomial Logistic Regression Aanalysis",
    "section": "2.2 Check the PRS effect (reproduce Table 2)",
    "text": "2.2 Check the PRS effect (reproduce Table 2)\n\n2.2.1 Implement in two binary logistic regression\n\n2.2.1.1 NGM vs. Prediabetes\n\n\nCode\nprediabetes &lt;- diabetes |&gt; filter(Diabetes %in% c(\"NGM\", \"Pre\"))\ndim(prediabetes)\n\n\n[1] 4889   20\n\n\nCode\ntable(prediabetes$Diabetes)\n\n\n\n NGM  Pre T2DM \n3944  945    0 \n\n\n\n\nCode\nprs3_bl_pre &lt;- glm(prs3, family = binomial(link = \"logit\"), data = prediabetes)\nsummary(prs3_bl_pre)\n\n\n\nCall:\nglm(formula = prs3, family = binomial(link = \"logit\"), data = prediabetes)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -8.004e+00  7.002e-01 -11.432  &lt; 2e-16 ***\nPRSMedium        3.017e-01  9.634e-02   3.131 0.001740 ** \nPRSHigh          4.352e-01  9.676e-02   4.498 6.85e-06 ***\nAge              4.734e-02  5.518e-03   8.579  &lt; 2e-16 ***\nSexFemale       -2.683e-01  9.477e-02  -2.831 0.004642 ** \nBMI              1.116e-01  1.070e-02  10.425  &lt; 2e-16 ***\nAlcohol          2.683e-03  3.236e-03   0.829 0.407153    \nSmokingFormer    9.964e-02  8.678e-02   1.148 0.250907    \nSmokingCurrent  -4.150e-02  1.396e-01  -0.297 0.766183    \nStep_wake       -1.806e-03  1.065e-03  -1.696 0.089835 .  \nDHD             -9.282e-03  2.966e-03  -3.129 0.001752 ** \nKCal            -1.092e-04  7.306e-05  -1.495 0.135044    \nCVDYes          -1.994e-01  1.157e-01  -1.722 0.085002 .  \nChol             9.577e-02  4.290e-02   2.232 0.025597 *  \nOMAP             1.457e-02  3.767e-03   3.869 0.000109 ***\nMed_HTYes        4.078e-01  9.238e-02   4.414 1.01e-05 ***\nMed_LPYes        6.212e-01  1.076e-01   5.774 7.75e-09 ***\nSugar           -3.767e-03  5.763e-03  -0.654 0.513348    \nB1_VD1No        -5.004e-01  9.011e-02  -5.553 2.80e-08 ***\nB1_VD1Unknow    -3.051e-01  1.245e-01  -2.450 0.014305 *  \nEducationMedium -2.258e-02  1.053e-01  -0.214 0.830219    \nEducationHigh   -6.935e-02  9.790e-02  -0.708 0.478690    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4800.6  on 4888  degrees of freedom\nResidual deviance: 4151.3  on 4867  degrees of freedom\nAIC: 4195.3\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nlogistic.display(prs3_bl_pre)\n\n\n\nLogistic regression predicting Diabetes \n \n                       crude OR(95%CI)         adj. OR(95%CI)         \nPRS: ref.=Low                                                         \n   Medium              1.36 (1.14,1.62)        1.35 (1.12,1.63)       \n   High                1.58 (1.33,1.89)        1.55 (1.28,1.87)       \n                                                                      \nAge (cont. var.)       1.06 (1.06,1.07)        1.05 (1.04,1.06)       \n                                                                      \nSex: Female vs Male    0.62 (0.54,0.72)        0.76 (0.64,0.92)       \n                                                                      \nBMI (cont. var.)       1.16 (1.14,1.18)        1.12 (1.09,1.14)       \n                                                                      \nAlcohol (cont. var.)   1.0125 (1.0073,1.0176)  1.0027 (0.9963,1.0091) \n                                                                      \nSmoking: ref.=Never                                                   \n   Former              1.52 (1.3,1.77)         1.1 (0.93,1.31)        \n   Current             1.17 (0.92,1.5)         0.96 (0.73,1.26)       \n                                                                      \nStep_wake (cont. var.) 0.9939 (0.992,0.9958)   0.9982 (0.9961,1.0003) \n                                                                      \nDHD (cont. var.)       0.9869 (0.9824,0.9915)  0.9908 (0.985,0.9965)  \n                                                                      \nKCal (cont. var.)      1 (0.9998,1.0001)       0.9999 (0.9997,1)      \n                                                                      \nCVD: Yes vs No         1.53 (1.27,1.86)        0.82 (0.65,1.03)       \n                                                                      \nChol (cont. var.)      0.93 (0.86,0.99)        1.1 (1.01,1.2)         \n                                                                      \nOMAP (cont. var.)      1.03 (1.03,1.04)        1.01 (1.01,1.02)       \n                                                                      \nMed_HT: Yes vs No      2.99 (2.58,3.47)        1.5 (1.25,1.8)         \n                                                                      \nMed_LP: Yes vs No      2.92 (2.49,3.43)        1.86 (1.51,2.3)        \n                                                                      \nSugar (cont. var.)     0.9994 (0.9893,1.0096)  0.9962 (0.985,1.0076)  \n                                                                      \nB1_VD1: ref.=Yes                                                      \n   No                  0.6 (0.51,0.7)          0.61 (0.51,0.72)       \n   Unknow              0.78 (0.62,0.97)        0.74 (0.58,0.94)       \n                                                                      \nEducation: ref.=Low                                                   \n   Medium              0.62 (0.51,0.74)        0.98 (0.8,1.2)         \n   High                0.58 (0.49,0.68)        0.93 (0.77,1.13)       \n                                                                      \n                       P(Wald's test) P(LR-test)\nPRS: ref.=Low                         &lt; 0.001   \n   Medium              0.002                    \n   High                &lt; 0.001                  \n                                                \nAge (cont. var.)       &lt; 0.001        &lt; 0.001   \n                                                \nSex: Female vs Male    0.005          0.005     \n                                                \nBMI (cont. var.)       &lt; 0.001        &lt; 0.001   \n                                                \nAlcohol (cont. var.)   0.407          0.409     \n                                                \nSmoking: ref.=Never                   0.377     \n   Former              0.251                    \n   Current             0.766                    \n                                                \nStep_wake (cont. var.) 0.09           0.089     \n                                                \nDHD (cont. var.)       0.002          0.002     \n                                                \nKCal (cont. var.)      0.135          0.134     \n                                                \nCVD: Yes vs No         0.085          0.082     \n                                                \nChol (cont. var.)      0.026          0.026     \n                                                \nOMAP (cont. var.)      &lt; 0.001        &lt; 0.001   \n                                                \nMed_HT: Yes vs No      &lt; 0.001        &lt; 0.001   \n                                                \nMed_LP: Yes vs No      &lt; 0.001        &lt; 0.001   \n                                                \nSugar (cont. var.)     0.513          0.51      \n                                                \nB1_VD1: ref.=Yes                      &lt; 0.001   \n   No                  &lt; 0.001                  \n   Unknow              0.014                    \n                                                \nEducation: ref.=Low                   0.768     \n   Medium              0.83                     \n   High                0.479                    \n                                                \nLog-likelihood = -2075.635\nNo. of observations = 4889\nAIC value = 4195.2701\n\n\n\n\n2.2.1.2 NGM vs. T2DS\n\n\nCode\nt2dm &lt;- diabetes |&gt; filter(Diabetes %in% c(\"NGM\", \"T2DM\"))\ndim(t2dm)\n\n\n[1] 5279   20\n\n\nCode\ntable(t2dm$Diabetes)\n\n\n\n NGM  Pre T2DM \n3944    0 1335 \n\n\n\n\nCode\nprs3_bl_t2dm &lt;- glm(prs3, family = binomial(link = \"logit\"), data = t2dm)\nsummary(prs3_bl_t2dm)\n\n\n\nCall:\nglm(formula = prs3, family = binomial(link = \"logit\"), data = t2dm)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -7.540e+00  8.516e-01  -8.854  &lt; 2e-16 ***\nPRSMedium        8.101e-01  1.240e-01   6.531 6.53e-11 ***\nPRSHigh          1.198e+00  1.215e-01   9.860  &lt; 2e-16 ***\nAge              5.366e-02  6.552e-03   8.189 2.63e-16 ***\nSexFemale       -8.024e-01  1.156e-01  -6.942 3.87e-12 ***\nBMI              1.741e-01  1.239e-02  14.055  &lt; 2e-16 ***\nAlcohol         -7.559e-03  3.878e-03  -1.949 0.051299 .  \nSmokingFormer    1.074e-01  1.051e-01   1.022 0.306925    \nSmokingCurrent   3.982e-01  1.571e-01   2.534 0.011270 *  \nStep_wake       -6.260e-03  1.294e-03  -4.836 1.33e-06 ***\nDHD             -1.135e-02  3.545e-03  -3.202 0.001363 ** \nKCal            -7.423e-05  8.491e-05  -0.874 0.382003    \nCVDYes          -4.544e-01  1.250e-01  -3.636 0.000277 ***\nChol            -4.487e-01  5.425e-02  -8.270  &lt; 2e-16 ***\nOMAP             2.329e-02  4.630e-03   5.030 4.90e-07 ***\nMed_HTYes        7.470e-01  1.046e-01   7.141 9.28e-13 ***\nMed_LPYes        1.473e+00  1.128e-01  13.058  &lt; 2e-16 ***\nSugar           -4.791e-02  8.685e-03  -5.516 3.47e-08 ***\nB1_VD1No        -1.620e+00  1.055e-01 -15.353  &lt; 2e-16 ***\nB1_VD1Unknow    -1.023e+00  1.399e-01  -7.317 2.54e-13 ***\nEducationMedium -1.917e-02  1.203e-01  -0.159 0.873423    \nEducationHigh   -2.835e-01  1.187e-01  -2.388 0.016922 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5970.4  on 5278  degrees of freedom\nResidual deviance: 3007.7  on 5257  degrees of freedom\nAIC: 3051.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\nlogistic.display(prs3_bl_t2dm)\n\n\n\nLogistic regression predicting Diabetes \n \n                       crude OR(95%CI)         adj. OR(95%CI)         \nPRS: ref.=Low                                                         \n   Medium              2.41 (2.02,2.86)        2.25 (1.76,2.87)       \n   High                3.49 (2.95,4.12)        3.31 (2.61,4.2)        \n                                                                      \nAge (cont. var.)       1.08 (1.07,1.09)        1.06 (1.04,1.07)       \n                                                                      \nSex: Female vs Male    0.34 (0.3,0.38)         0.45 (0.36,0.56)       \n                                                                      \nBMI (cont. var.)       1.28 (1.26,1.31)        1.19 (1.16,1.22)       \n                                                                      \nAlcohol (cont. var.)   0.9985 (0.9936,1.0033)  0.9925 (0.985,1)       \n                                                                      \nSmoking: ref.=Never                                                   \n   Former              1.72 (1.49,1.97)        1.11 (0.91,1.37)       \n   Current             1.68 (1.37,2.05)        1.49 (1.09,2.03)       \n                                                                      \nStep_wake (cont. var.) 0.9836 (0.9818,0.9854)  0.9938 (0.9912,0.9963) \n                                                                      \nDHD (cont. var.)       0.98 (0.97,0.98)        0.99 (0.98,1)          \n                                                                      \nKCal (cont. var.)      0.9999 (0.9998,1)       0.9999 (0.9998,1.0001) \n                                                                      \nCVD: Yes vs No         2.71 (2.33,3.16)        0.63 (0.5,0.81)        \n                                                                      \nChol (cont. var.)      0.35 (0.33,0.38)        0.64 (0.57,0.71)       \n                                                                      \nOMAP (cont. var.)      1.05 (1.04,1.05)        1.02 (1.01,1.03)       \n                                                                      \nMed_HT: Yes vs No      8.82 (7.67,10.15)       2.11 (1.72,2.59)       \n                                                                      \nMed_LP: Yes vs No      14.31 (12.35,16.59)     4.36 (3.5,5.44)        \n                                                                      \nSugar (cont. var.)     0.97 (0.95,0.98)        0.95 (0.94,0.97)       \n                                                                      \nB1_VD1: ref.=Yes                                                      \n   No                  0.19 (0.16,0.22)        0.2 (0.16,0.24)        \n   Unknow              0.41 (0.34,0.5)         0.36 (0.27,0.47)       \n                                                                      \nEducation: ref.=Low                                                   \n   Medium              0.57 (0.49,0.67)        0.98 (0.77,1.24)       \n   High                0.36 (0.31,0.42)        0.75 (0.6,0.95)        \n                                                                      \n                       P(Wald's test) P(LR-test)\nPRS: ref.=Low                         &lt; 0.001   \n   Medium              &lt; 0.001                  \n   High                &lt; 0.001                  \n                                                \nAge (cont. var.)       &lt; 0.001        &lt; 0.001   \n                                                \nSex: Female vs Male    &lt; 0.001        &lt; 0.001   \n                                                \nBMI (cont. var.)       &lt; 0.001        &lt; 0.001   \n                                                \nAlcohol (cont. var.)   0.051          0.05      \n                                                \nSmoking: ref.=Never                   0.041     \n   Former              0.307                    \n   Current             0.011                    \n                                                \nStep_wake (cont. var.) &lt; 0.001        &lt; 0.001   \n                                                \nDHD (cont. var.)       0.001          0.001     \n                                                \nKCal (cont. var.)      0.382          0.382     \n                                                \nCVD: Yes vs No         &lt; 0.001        &lt; 0.001   \n                                                \nChol (cont. var.)      &lt; 0.001        &lt; 0.001   \n                                                \nOMAP (cont. var.)      &lt; 0.001        &lt; 0.001   \n                                                \nMed_HT: Yes vs No      &lt; 0.001        &lt; 0.001   \n                                                \nMed_LP: Yes vs No      &lt; 0.001        &lt; 0.001   \n                                                \nSugar (cont. var.)     &lt; 0.001        &lt; 0.001   \n                                                \nB1_VD1: ref.=Yes                      &lt; 0.001   \n   No                  &lt; 0.001                  \n   Unknow              &lt; 0.001                  \n                                                \nEducation: ref.=Low                   0.03      \n   Medium              0.873                    \n   High                0.017                    \n                                                \nLog-likelihood = -1503.8424\nNo. of observations = 5279\nAIC value = 3051.6849\n\n\n\n\n\n2.2.2 Implement in a multinomial logistic regression\n\n\nCode\nprs3_ml &lt;- multinom(prs3, data = diabetes)\n\n\n# weights:  69 (44 variable)\ninitial  value 6837.762885 \niter  10 value 5215.529946\niter  20 value 4639.168404\niter  30 value 4273.319886\niter  40 value 4017.408371\niter  50 value 3983.845641\nfinal  value 3983.621664 \nconverged\n\n\nCode\nsummary(prs3_ml)\n\n\nCall:\nmultinom(formula = prs3, data = diabetes)\n\nCoefficients:\n     (Intercept) PRSMedium   PRSHigh        Age SexFemale       BMI\nPre    -7.993443 0.3048323 0.4590396 0.04691694 -0.269743 0.1105872\nT2DM   -7.530014 0.8205179 1.1457251 0.04937704 -0.845876 0.1727318\n          Alcohol SmokingFormer SmokingCurrent    Step_wake          DHD\nPre   0.003132189     0.1205212     0.03541554 -0.001591920 -0.008673513\nT2DM -0.008483711     0.1014127     0.28413325 -0.007081803 -0.008881604\n              KCal     CVDYes        Chol       OMAP Med_HTYes Med_LPYes\nPre  -1.252987e-04 -0.2092194  0.09181539 0.01470355 0.4051368  0.647377\nT2DM -8.870268e-05 -0.5104388 -0.41867134 0.02422280 0.7199281  1.508455\n            Sugar   B1_VD1No B1_VD1Unknow EducationMedium EducationHigh\nPre  -0.004413489 -0.4966035   -0.3253156     -0.06624586    -0.1073260\nT2DM -0.044371036 -1.6066637   -0.9962776      0.04357005    -0.2772149\n\nStd. Errors:\n     (Intercept)  PRSMedium    PRSHigh         Age  SexFemale         BMI\nPre  0.005541274 0.08199097 0.08180552 0.004718533 0.08838194 0.009288211\nT2DM 0.004586410 0.06104669 0.06058859 0.005198003 0.09790667 0.010027835\n         Alcohol SmokingFormer SmokingCurrent   Step_wake         DHD\nPre  0.003071018    0.07749479     0.09140973 0.001011571 0.002726991\nT2DM 0.003585977    0.08396675     0.07539267 0.001174780 0.003033971\n             KCal     CVDYes       Chol        OMAP  Med_HTYes  Med_LPYes\nPre  6.883390e-05 0.07184194 0.03926264 0.003353974 0.07653429 0.07178370\nT2DM 7.598204e-05 0.06452109 0.04711608 0.003757188 0.07818381 0.06507821\n           Sugar  B1_VD1No B1_VD1Unknow EducationMedium EducationHigh\nPre  0.005542964 0.0740401   0.08135444      0.07884324    0.07481118\nT2DM 0.008251040 0.0811196   0.07534407      0.07216372    0.07256924\n\nResidual Deviance: 7967.243 \nAIC: 8055.243 \n\n\nCode\nmlogit.display(prs3_ml)\n\n\n\nOutcome =Diabetes; Referent group = NGM \n                Pre                            T2DM                          \n                Coeff./SE      RRR(95%CI)      Coeff./SE      RRR(95%CI)     \n(Intercept)     -7.99/0.006NA  -               -7.53/0.005NA  -              \nPRSMedium       0.3/0.082***   1.36(1.16,1.59) 0.82/0.061***  2.27(2.02,2.56)\nPRSHigh         0.46/0.082***  1.58(1.35,1.86) 1.15/0.061***  3.14(2.79,3.54)\nAge             0.05/0.005***  1.05(1.04,1.06) 0.05/0.005***  1.05(1.04,1.06)\nSexFemale       -0.27/0.088**  0.76(0.64,0.91) -0.85/0.098*** 0.43(0.35,0.52)\nBMI             0.11/0.009***  1.12(1.1,1.14)  0.17/0.01***   1.19(1.17,1.21)\nAlcohol         0/0.003        1(1,1.01)       -0.01/0.004*   0.99(0.98,1)   \nSmokingFormer   0.12/0.077     1.13(0.97,1.31) 0.1/0.084      1.11(0.94,1.3) \nSmokingCurrent  0.04/0.091     1.04(0.87,1.24) 0.28/0.075***  1.33(1.15,1.54)\nStep_wake       0/0.001        1(1,1)          -0.01/0.001*** 0.99(0.99,1)   \nDHD             -0.01/0.003**  0.99(0.99,1)    -0.01/0.003**  0.99(0.99,1)   \nKCal            0/0            1(1,1)          0/0            1(1,1)         \nCVDYes          -0.21/0.072**  0.81(0.7,0.93)  -0.51/0.065*** 0.6(0.53,0.68) \nChol            0.09/0.039*    1.1(1.01,1.18)  -0.42/0.047*** 0.66(0.6,0.72) \nOMAP            0.01/0.003***  1.01(1.01,1.02) 0.02/0.004***  1.02(1.02,1.03)\nMed_HTYes       0.41/0.077***  1.5(1.29,1.74)  0.72/0.078***  2.05(1.76,2.39)\nMed_LPYes       0.65/0.072***  1.91(1.66,2.2)  1.51/0.065***  4.52(3.98,5.13)\nSugar           0/0.006        1(0.98,1.01)    -0.04/0.008*** 0.96(0.94,0.97)\nB1_VD1No        -0.5/0.074***  0.61(0.53,0.7)  -1.61/0.081*** 0.2(0.17,0.24) \nB1_VD1Unknow    -0.33/0.081*** 0.72(0.62,0.85) -1/0.075***    0.37(0.32,0.43)\nEducationMedium -0.07/0.079    0.94(0.8,1.09)  0.04/0.072     1.04(0.91,1.2) \nEducationHigh   -0.11/0.075    0.9(0.78,1.04)  -0.28/0.073*** 0.76(0.66,0.87)\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  \n\nResidual Deviance: 7967.24 \nAIC = 8055.24 \n\n\nThe results from a multinomial logistic model is almost the same as the results getting from two binary logistic models.\n\nIssue of selecting one multinomial logistic or two binary logistic\n\nData use\nVariable exclusive -&gt; predicition\n\n\n\n\n\nCode\nprs1_ml &lt;- multinom(prs1, data = diabetes)\n\n\n# weights:  18 (10 variable)\ninitial  value 6837.762885 \niter  10 value 5404.157593\nfinal  value 5164.014496 \nconverged\n\n\nCode\nprs2_ml &lt;- multinom(prs2, data = diabetes)\n\n\n# weights:  39 (24 variable)\ninitial  value 6837.762885 \niter  10 value 5290.331013\niter  20 value 4842.276876\niter  30 value 4605.592708\nfinal  value 4605.590003 \nconverged\n\n\nCode\nlrtest(prs1_ml, prs2_ml, prs3_ml)\n\n\nLikelihood ratio test\n\nModel 1: Diabetes ~ PRS + Age + Sex\nModel 2: Diabetes ~ PRS + Age + Sex + BMI + Alcohol + Smoking + Step_wake + \n    DHD + KCal\nModel 3: Diabetes ~ PRS + Age + Sex + BMI + Alcohol + Smoking + Step_wake + \n    DHD + KCal + CVD + Chol + OMAP + Med_HT + Med_LP + Sugar + \n    B1_VD1 + Education\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1  10 -5164.0                         \n2  24 -4605.6 14 1116.8  &lt; 2.2e-16 ***\n3  44 -3983.6 20 1243.9  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nget_OR_CI &lt;- function(multi_model, DV_level, EV_level, alpha = 0.05) {\n  esti &lt;- coef(multi_model)[DV_level, , drop = FALSE]\n  .cov_mat &lt;- vcov(multi_model)\n  .select_variable &lt;- str_detect(colnames(.cov_mat), DV_level)\n  cov_mat &lt;- .cov_mat[.select_variable, .select_variable]\n  \n  cont &lt;- matrix(multi_model$coefnames %in% EV_level, nrow = 1)\n  l &lt;- cont %*% t(esti)\n  OR &lt;- exp(l) |&gt; round(2)\n  SE &lt;- sqrt(cont %*% cov_mat %*% t(cont))\n  lower_CI &lt;- exp(l - qnorm(1-alpha/2) * SE) |&gt; round(2)\n  upper_CI &lt;- exp(l + qnorm(1-alpha/2) * SE) |&gt; round(2)\n  \n  str_glue(\"{OR} &lt;br&gt; ({lower_CI}, {upper_CI})\")\n}\n\n\n\n\nCode\nlevels &lt;- c(\"Low\", \"Medium\", \"High\") \nPRSlevels &lt;- str_glue(\"PRS{levels}\")\n\nprs_OR_table &lt;- tibble(\n  level = levels ,\n  Pre_prs1 = map_chr(PRSlevels, ~ get_OR_CI(prs1_ml, \"Pre\", .)),\n  Pre_prs2 = map_chr(PRSlevels, ~ get_OR_CI(prs2_ml, \"Pre\", .)),\n  Pre_prs3 = map_chr(PRSlevels, ~ get_OR_CI(prs3_ml, \"Pre\", .)),\n  T2DM_prs1 = map_chr(PRSlevels, ~ get_OR_CI(prs1_ml, \"T2DM\", .)),\n  T2DM_prs2 = map_chr(PRSlevels, ~ get_OR_CI(prs2_ml, \"T2DM\", .)),\n  T2DM_prs3 = map_chr(PRSlevels, ~ get_OR_CI(prs3_ml, \"T2DM\", .)))\n\ngt(prs_OR_table, rowname_col = \"level\") |&gt;\n  tab_stubhead(label = \"PRS\") |&gt;\n  tab_spanner(label = \"Pre\", columns = starts_with(\"Pre\")) |&gt;\n  tab_spanner(label = \"T2DM\", columns = starts_with(\"T2DM\")) |&gt;\n  cols_label(ends_with(\"1\") ~ \"M1\",\n             ends_with(\"2\") ~ \"M2\",\n             ends_with(\"3\") ~ \"M3\") |&gt;\n  fmt_markdown()\n\n\n\n\nTable 2: Association between PRS and Prediabetes/T2DM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPRS\n\nPre\n\n\nT2DM\n\n\n\nM1\nM2\nM3\nM1\nM2\nM3\n\n\n\n\nLow\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n\n\nMedium\n1.38  (1.15, 1.65)\n1.4  (1.21, 1.62)\n1.36  (1.16, 1.59)\n2.45  (2.05, 2.94)\n2.54  (2.26, 2.86)\n2.27  (2.02, 2.56)\n\n\nHigh\n1.66  (1.39, 1.99)\n1.74  (1.5, 2.01)\n1.58  (1.35, 1.86)\n3.76  (3.16, 4.49)\n4.18  (3.72, 4.7)\n3.14  (2.79, 3.54)"
  },
  {
    "objectID": "posts/20240918_CDP/index.html#check-the-prs-effect-reproduce-table-3",
    "href": "posts/20240918_CDP/index.html#check-the-prs-effect-reproduce-table-3",
    "title": "Multinomial Logistic Regression Aanalysis",
    "section": "2.3 Check the PRS effect (reproduce Table 3)",
    "text": "2.3 Check the PRS effect (reproduce Table 3)\n\n\nCode\ncof1_ml &lt;- multinom(cof1, data = diabetes)\n\n\n# weights:  24 (14 variable)\ninitial  value 6837.762885 \niter  10 value 5398.637887\niter  20 value 5159.349831\nfinal  value 5159.349135 \nconverged\n\n\nCode\ncof2_ml &lt;- multinom(cof2, data = diabetes)\n\n\n# weights:  45 (28 variable)\ninitial  value 6837.762885 \niter  10 value 5290.261102\niter  20 value 4845.902800\niter  30 value 4641.391554\nfinal  value 4601.460847 \nconverged\n\n\nCode\ncof3_ml &lt;- multinom(cof3, data = diabetes)\n\n\n# weights:  75 (48 variable)\ninitial  value 6837.762885 \niter  10 value 5215.456459\niter  20 value 4635.906612\niter  30 value 4251.564366\niter  40 value 4014.807109\niter  50 value 3985.463064\nfinal  value 3978.814138 \nconverged\n\n\n\n\nCode\nCoflevels &lt;- str_glue(\"Coffee{levels}\")\n\ncof_OR_table &lt;- tibble(\n  level = levels ,\n  Pre_cof1 = map_chr(Coflevels, ~ get_OR_CI(cof1_ml, \"Pre\", .)),\n  Pre_cof2 = map_chr(Coflevels, ~ get_OR_CI(cof2_ml, \"Pre\", .)),\n  Pre_cof3 = map_chr(Coflevels, ~ get_OR_CI(cof3_ml, \"Pre\", .)),\n  T2DM_cof1 = map_chr(Coflevels, ~ get_OR_CI(cof1_ml, \"T2DM\", .)),\n  T2DM_cof2 = map_chr(Coflevels, ~ get_OR_CI(cof2_ml, \"T2DM\", .)),\n  T2DM_cof3 = map_chr(Coflevels, ~ get_OR_CI(cof3_ml, \"T2DM\", .)))\n\ngt(cof_OR_table, rowname_col = \"level\") |&gt;\n  tab_stubhead(label = \"Coffee\") |&gt;\n  tab_spanner(label = \"Pre\", columns = starts_with(\"Pre\")) |&gt;\n  tab_spanner(label = \"T2DM\", columns = starts_with(\"T2DM\")) |&gt;\n  cols_label(ends_with(\"1\") ~ \"M1\",\n             ends_with(\"2\") ~ \"M2\",\n             ends_with(\"3\") ~ \"M3\") |&gt;\n  fmt_markdown()\n\n\n\n\nTable 3: Association between Coffee and Prediabetes/T2DM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee\n\nPre\n\n\nT2DM\n\n\n\nM1\nM2\nM3\nM1\nM2\nM3\n\n\n\n\nLow\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n\n\nMedium\n0.81  (0.68, 0.97)\n0.8  (0.7, 0.92)\n0.8  (0.69, 0.92)\n0.82  (0.7, 0.97)\n0.83  (0.73, 0.94)\n0.8  (0.7, 0.91)\n\n\nHigh\n0.9  (0.75, 1.08)\n0.85  (0.74, 0.97)\n0.83  (0.72, 0.96)\n0.97  (0.82, 1.14)\n0.87  (0.76, 0.99)\n0.8  (0.7, 0.91)"
  },
  {
    "objectID": "posts/20240918_CDP/index.html#check-the-prs-and-coffee-interaction-reproduce-table-4",
    "href": "posts/20240918_CDP/index.html#check-the-prs-and-coffee-interaction-reproduce-table-4",
    "title": "Multinomial Logistic Regression Aanalysis",
    "section": "2.4 Check the PRS and coffee interaction (reproduce Table 4)",
    "text": "2.4 Check the PRS and coffee interaction (reproduce Table 4)\n\n\nCode\nprs_cof1_ml &lt;- multinom(prs_cof1, data = diabetes)\n\n\n# weights:  24 (14 variable)\ninitial  value 6837.762885 \niter  10 value 5398.637887\niter  20 value 5159.349831\nfinal  value 5159.349135 \nconverged\n\n\nCode\nprs_cof_i1_ml &lt;- multinom(prs_cof_i1, data = diabetes)\n\n\n# weights:  36 (22 variable)\ninitial  value 6837.762885 \niter  10 value 5465.181242\niter  20 value 5199.344506\niter  30 value 5154.011791\niter  30 value 5154.011788\niter  30 value 5154.011788\nfinal  value 5154.011788 \nconverged\n\n\nCode\nprs_cof1_ml\n\n\nCall:\nmultinom(formula = prs_cof1, data = diabetes)\n\nCoefficients:\n     (Intercept) CoffeeMedium  CoffeeHigh PRSMedium   PRSHigh        Age\nPre    -5.127968   -0.2087765 -0.10369891 0.3228881 0.5081659 0.06163677\nT2DM   -5.901011   -0.1941127 -0.03432375 0.8988806 1.3236497 0.07414162\n      SexFemale\nPre  -0.3943786\nT2DM -1.0023219\n\nResidual Deviance: 10318.7 \nAIC: 10346.7 \n\n\nCode\nmlogit.display(prs_cof1_ml)\n\n\n\nOutcome =Diabetes; Referent group = NGM \n             Pre                            T2DM                         \n             Coeff./SE      RRR(95%CI)      Coeff./SE     RRR(95%CI)     \n(Intercept)  -5.13/0.305*** -               -5.9/0.29***  -              \nCoffeeMedium -0.21/0.089*   0.81(0.68,0.97) -0.19/0.083*  0.82(0.7,0.97) \nCoffeeHigh   -0.1/0.091     0.9(0.75,1.08)  -0.03/0.083   0.97(0.82,1.14)\nPRSMedium    0.32/0.092***  1.38(1.15,1.65) 0.9/0.092***  2.46(2.05,2.94)\nPRSHigh      0.51/0.091***  1.66(1.39,1.99) 1.32/0.09***  3.76(3.15,4.48)\nAge          0.06/0.005***  1.06(1.05,1.07) 0.07/0.004*** 1.08(1.07,1.09)\nSexFemale    -0.39/0.075*** 0.67(0.58,0.78) -1/0.071***   0.37(0.32,0.42)\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  \n\nResidual Deviance: 10318.7 \nAIC = 10346.7 \n\n\nCode\nmlogit.display(prs_cof_i1_ml)\n\n\n\nOutcome =Diabetes; Referent group = NGM \n                       Pre                            T2DM          \n                       Coeff./SE      RRR(95%CI)      Coeff./SE     \n(Intercept)            -5.13/0.311*** -               -5.75/0.297***\nCoffeeMedium           -0.31/0.159*   0.73(0.53,1)    -0.5/0.174**  \nCoffeeHigh             -0.01/0.158    0.99(0.72,1.34) -0.26/0.174   \nPRSMedium              0.28/0.148     1.32(0.99,1.76) 0.76/0.146*** \nPRSHigh                0.52/0.144***  1.68(1.27,2.23) 1.06/0.142*** \nAge                    0.06/0.005***  1.06(1.05,1.07) 0.07/0.004*** \nSexFemale              -0.39/0.075*** 0.67(0.58,0.78) -1/0.071***   \nCoffeeMedium:PRSMedium 0.14/0.221     1.16(0.75,1.78) 0.24/0.222    \nCoffeeHigh:PRSMedium   0/0.222        1(0.65,1.54)    0.23/0.222    \nCoffeeMedium:PRSHigh   0.19/0.219     1.21(0.79,1.86) 0.54/0.217*   \nCoffeeHigh:PRSHigh     -0.24/0.221    0.79(0.51,1.21) 0.33/0.215    \n                                      \n                       RRR(95%CI)     \n(Intercept)            -              \nCoffeeMedium           0.61(0.43,0.85)\nCoffeeHigh             0.77(0.55,1.08)\nPRSMedium              2.13(1.6,2.84) \nPRSHigh                2.88(2.18,3.8) \nAge                    1.08(1.07,1.09)\nSexFemale              0.37(0.32,0.42)\nCoffeeMedium:PRSMedium 1.27(0.82,1.97)\nCoffeeHigh:PRSMedium   1.26(0.81,1.94)\nCoffeeMedium:PRSHigh   1.71(1.12,2.61)\nCoffeeHigh:PRSHigh     1.4(0.92,2.13) \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  \n\nResidual Deviance: 10308.02 \nAIC = 10352.02 \n\n\nCode\nlrtest(prs_cof1_ml, prs_cof_i1_ml)\n\n\nLikelihood ratio test\n\nModel 1: Diabetes ~ Coffee + PRS + Age + Sex\nModel 2: Diabetes ~ Coffee + PRS + Age + Sex + PRS:Coffee\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1  14 -5159.3                     \n2  22 -5154.0  8 10.675     0.2208\n\n\nCode\nprs_cof2_ml &lt;- multinom(prs_cof2, data = diabetes)\n\n\n# weights:  45 (28 variable)\ninitial  value 6837.762885 \niter  10 value 5290.261102\niter  20 value 4845.902800\niter  30 value 4641.391554\nfinal  value 4601.460847 \nconverged\n\n\nCode\nprs_cof_i2_ml &lt;- multinom(prs_cof_i2, data = diabetes)\n\n\n# weights:  57 (36 variable)\ninitial  value 6837.762885 \niter  10 value 5289.996854\niter  20 value 4847.821658\niter  30 value 4690.907601\niter  40 value 4596.380911\nfinal  value 4595.158389 \nconverged\n\n\nCode\nlrtest(prs_cof2_ml, prs_cof_i2_ml)\n\n\nLikelihood ratio test\n\nModel 1: Diabetes ~ Coffee + PRS + Age + Sex + BMI + Alcohol + Smoking + \n    Step_wake + DHD + KCal\nModel 2: Diabetes ~ Coffee + PRS + Age + Sex + BMI + Alcohol + Smoking + \n    Step_wake + DHD + KCal + PRS:Coffee\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1  28 -4601.5                     \n2  36 -4595.2  8 12.605     0.1262\n\n\nCode\nprs_cof3_ml &lt;- multinom(prs_cof3, data = diabetes)\n\n\n# weights:  75 (48 variable)\ninitial  value 6837.762885 \niter  10 value 5215.456459\niter  20 value 4635.906612\niter  30 value 4251.564366\niter  40 value 4014.807109\niter  50 value 3985.463064\nfinal  value 3978.814138 \nconverged\n\n\nCode\nprs_cof_i3_ml &lt;- multinom(prs_cof_i3, data = diabetes)\n\n\n# weights:  87 (56 variable)\ninitial  value 6837.762885 \niter  10 value 5215.195654\niter  20 value 4636.177402\niter  30 value 4155.562151\niter  40 value 4066.224971\niter  50 value 3998.661537\niter  60 value 3975.634345\nfinal  value 3973.411479 \nconverged\n\n\nCode\nlrtest(prs_cof3_ml, prs_cof_i3_ml)\n\n\nLikelihood ratio test\n\nModel 1: Diabetes ~ Coffee + PRS + Age + Sex + BMI + Alcohol + Smoking + \n    Step_wake + DHD + KCal + CVD + Chol + OMAP + Med_HT + Med_LP + \n    Sugar + B1_VD1 + Education\nModel 2: Diabetes ~ Coffee + PRS + Age + Sex + BMI + Alcohol + Smoking + \n    Step_wake + DHD + KCal + CVD + Chol + OMAP + Med_HT + Med_LP + \n    Sugar + B1_VD1 + Education + PRS:Coffee\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1  48 -3978.8                     \n2  56 -3973.4  8 10.805      0.213\n\n\nIn each case, the model with and without interaction terms (between Coffee and PRS) are not statistical significant. Thus, we would choose the model without interaction terms (i.e., prs_cof1, prs_cof2, and prs_cof3) as the final results.\nNow, I reproduce Table 4 in the paper\nTo illustrate how to calculate the odds(-like) ratios in two categorical exposure variabes, I take Model prs_cof1 for example. The model for Pre vs. NGM is\n\\[\n\\ln\\left(\\frac{p_{\\text{Pre}}}{p_{\\text{NGM}}} \\right) = \\alpha_{11} + \\beta_{11} \\text{CoffeeMedium} + \\beta_{12} \\text{CoffeeHigh} + \\beta_{13} \\text{PRSMedium} + \\beta_{14} \\text{PRSHigh} + \\gamma_{11} \\text{Age} + \\gamma_{12} \\text{SexFemale}\n\\]\nLet $l_{} = $ , where \\(C \\in \\{\\}\\) for Coffee and \\(Y = Y The general odds ratio formula is:\\)$ OR = e^l. \\[\nand the 95% CI of OR is:\n\\] (1-)%~CI = e^{l Z_{1-/2}} \\[\nwhere the variance of $l$ is\n\\] Var(l) = $$\nIf I want to know the OR when Coffe level is medium and PRS level is high, it means\n\\[\n\\text{CoffeeMedium} = \\text{PRSHigh} = 1 \\quad \\& \\quad \\text{CoffeeHight} = \\text{PRSMedium} = 0\n\\]\n\\[\nOR_{MH} = e^{\\beta{11} + \\beta{14}}\n\\]\nwe let …..\n\n\nCode\nPRSlevels = paste0(\"PRS\", rep(levels, times = 3))\nCoflevels = paste0(\"Coffee\", rep(levels, each = 3))\n\nprs_cof_OR_table &lt;- tibble(\n  Cof_levels = rep(levels, each = 3),\n  PRS_levels = rep(levels, times = 3),\n  Pre_prs_cof1 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof1_ml, \"Pre\", c(.x, .y))),\n  Pre_prs_cof2 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof2_ml, \"Pre\", c(.x, .y))),\n  Pre_prs_cof3 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof3_ml, \"Pre\", c(.x, .y))),\n  T2DM_prs_cof1 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof1_ml, \"T2DM\", c(.x, .y))),\n  T2DM_prs_cof2 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof2_ml, \"T2DM\", c(.x, .y))),\n  T2DM_prs_cof3 = map2_chr(PRSlevels, Coflevels, ~ get_OR_CI(prs_cof3_ml, \"T2DM\", c(.x, .y))))\n\ngt(prs_cof_OR_table,\n   rowname_col = \"PRS_level\", \n   groupname_col = \"Cof_levels\", \n   row_group_as_column = TRUE) |&gt;\n  tab_stubhead(label = c(\"Coffee\", \"PRS\")) |&gt;\n  tab_spanner(label = \"Pre\", columns = starts_with(\"Pre\")) |&gt;\n  tab_spanner(label = \"T2DM\", columns = starts_with(\"T2DM\")) |&gt;\n  cols_label(ends_with(\"1\") ~ \"M1\",\n             ends_with(\"2\") ~ \"M2\",\n             ends_with(\"3\") ~ \"M3\") |&gt;\n  fmt_markdown()\n\n\n\n\nTable 4: Association between PRES, Coffee and Prediabetes/T2DM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee\nPRS\n\nPre\n\n\nT2DM\n\n\n\nM1\nM2\nM3\nM1\nM2\nM3\n\n\n\n\nLow\nLow\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n1  (1, 1)\n\n\nMedium\n1.38  (1.15, 1.65)\n1.4  (1.21, 1.63)\n1.36  (1.16, 1.59)\n2.46  (2.05, 2.94)\n2.55  (2.26, 2.87)\n2.28  (2.02, 2.57)\n\n\nHigh\n1.66  (1.39, 1.99)\n1.74  (1.5, 2.01)\n1.58  (1.35, 1.86)\n3.76  (3.15, 4.48)\n4.19  (3.73, 4.71)\n3.16  (2.8, 3.56)\n\n\nMedium\nLow\n0.81  (0.68, 0.97)\n0.8  (0.7, 0.92)\n0.8  (0.69, 0.92)\n0.82  (0.7, 0.97)\n0.83  (0.73, 0.94)\n0.8  (0.7, 0.91)\n\n\nMedium\n1.12  (0.87, 1.44)\n1.13  (0.93, 1.37)\n1.08  (0.88, 1.34)\n2.02  (1.59, 2.57)\n2.11  (1.78, 2.51)\n1.82  (1.52, 2.17)\n\n\nHigh\n1.35  (1.05, 1.73)\n1.4  (1.15, 1.7)\n1.26  (1.02, 1.56)\n3.09  (2.44, 3.93)\n3.47  (2.92, 4.13)\n2.51  (2.1, 3.01)\n\n\nHigh\nLow\n0.9  (0.75, 1.08)\n0.85  (0.74, 0.97)\n0.83  (0.72, 0.96)\n0.97  (0.82, 1.14)\n0.87  (0.76, 0.99)\n0.8  (0.7, 0.91)\n\n\nMedium\n1.25  (0.97, 1.6)\n1.19  (0.97, 1.45)\n1.13  (0.91, 1.39)\n2.37  (1.87, 3.02)\n2.22  (1.86, 2.64)\n1.82  (1.53, 2.18)\n\n\nHigh\n1.5  (1.17, 1.92)\n1.47  (1.21, 1.79)\n1.31  (1.06, 1.62)\n3.63  (2.86, 4.6)\n3.64  (3.07, 4.32)\n2.53  (2.12, 3.01)\n\n\n\n\n\n\n\n\n\n\nIt is not necessary to strtify the data into differnt data sets and then do logistic analysis for each."
  },
  {
    "objectID": "posts/20250330_CCTblog/index.html",
    "href": "posts/20250330_CCTblog/index.html",
    "title": "Building a website to record your research",
    "section": "",
    "text": "時間可能得追逤 2021 年 …\nhttps://cct.netlify.app/"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTzu-Yao Lin\n\n\nJul 3, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/presentation.html#section-1",
    "href": "research/presentation.html#section-1",
    "title": "Presentations",
    "section": "2024",
    "text": "2024\nConference: IMPS 2024 - Location: Prague University, Czech Republic - Time: 2024-07-16 ~ 2024-07-19 - Title: Reliability in intensive longitudinal studies: A state-space vs. linear mixed model approach - Slide:\n\n  \n    This browser does not support PDFs. Please download the PDF to view it: \n      Download PDF\n      ."
  },
  {
    "objectID": "research/presentation.html#section-2",
    "href": "research/presentation.html#section-2",
    "title": "Presentations",
    "section": "2023",
    "text": "2023\nConference: The 10th European Congress of Methodology (EAM 2023) - Location: Ghent University, Belgium - Time: 10-07-2023 ~ 13-07-2023 - Title: Reliability for multilevel data: A correlation approach - Slide:\n\n  \n    This browser does not support PDFs. Please download the PDF to view it: \n      Download PDF\n      .\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConference: MathPsych/ICCM/EMPG 2023 - Location: University of Amsterdam, the Netherlands - Time: 18-07-2023 ~ 21-07-2023 - Title: Incorporating the Luce-Krantz threshold model into the cultural consensus theory for ordinal categorical data: A simulation study - Slide:\n   \n       \n    This browser does not support PDFs. Please download the PDF to view it:          Download PDF       \n      ."
  }
]